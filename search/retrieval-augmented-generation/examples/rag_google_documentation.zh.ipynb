{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z3Gi-zdCeEbE"
      },
      "outputs": [],
      "source": [
        "# Copyright 2023 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcfbGhs1eJF6"
      },
      "source": [
        "# 文件問答與檢索增強生成\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/doggy8088/generative-ai/blob/main/search/retrieval-augmented-generation/examples/rag_google_documentation.zh.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Google Colaboratory logo\"><br> Run in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/doggy8088/generative-ai/blob/main/search/retrieval-augmented-generation/examples/rag_google_documentation.zh.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/doggy8088/generative-ai/blob/main/search/retrieval-augmented-generation/examples/rag_google_documentation.zh.ipynb\">\n",
        "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghQ2aBsbnMyn"
      },
      "source": [
        "---\n",
        "\n",
        "* 作者：Gabe Rives-Corbett\n",
        "\n",
        "---\n",
        "\n",
        "此筆記本展示如何執行檢索增強生成，並進行基本的自動化評估。它展示了區塊大小、重疊和上下文長度對模型輸出的影響。此筆記本將建立一個問答系統，可讓你根據 Google Cloud Generative AI 文件尋找資訊。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsW5tPDRkT4m"
      },
      "source": [
        "## 開始使用\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jx1FQVAokWVb"
      },
      "source": [
        "### 安裝函式庫\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nJFw23w1kYVj"
      },
      "outputs": [],
      "source": [
        "%pip install -q --upgrade --user google-cloud-aiplatform==1.36.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5Xep4W9lq-Z"
      },
      "source": [
        "### 重新啟動目前的執行階段\n",
        "\n",
        "要在此 Jupyter 執行階段中使用新安裝的套件，你必須重新啟動執行階段。你可以執行下列Cell來執行此項操作，如此將重新啟動目前的Kernel。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XRvKdaPDTznN",
        "outputId": "154a71b5-f302-4f53-ed2f-b3e5fef9195b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'status': 'ok', 'restart': True}"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Restart kernel after installs so that your environment can access the new packages\n",
        "import IPython\n",
        "import time\n",
        "\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbmM4z7FOBpM"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "<b>⚠️ Kernel將重新啟動。請等待它完成，再繼續執行下一個步驟。⚠️</b>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jwsaMQYkZm8"
      },
      "source": [
        "### 認證你的 notebook 環境 (僅限 Colab) \n",
        "\n",
        "如果你是在 Google Colab 上執行這個筆記本，你將需要認證你的環境。為執行這項工作，請執行下列新的Cell。如果你使用的是 [Vertex AI Workbench](https://cloud.google.com/vertex-ai-workbench)，則不需要執行這個步驟。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ikOmH4doxOFs"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    # Authenticate user to Google Cloud\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-h0ba4rmkpKW"
      },
      "source": [
        "### 匯入函式庫\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YLUml_s7iqBc"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import itertools\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import numpy.linalg\n",
        "import vertexai\n",
        "\n",
        "from google.api_core import retry\n",
        "from vertexai.language_models import TextEmbeddingModel, TextGenerationModel\n",
        "from tqdm.auto import tqdm\n",
        "from bs4 import BeautifulSoup, Tag\n",
        "\n",
        "tqdm.pandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 設定筆記本環境\n",
        "\n",
        "### 設定以下常數以反映你的環境\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define project information\n",
        "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\n",
        "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
        "\n",
        "# Initialize Vertex AI SDK\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKBmi2BMk_OU"
      },
      "source": [
        "## 從 Google Cloud 文件中擷取文字\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXG6N0WclGsQ"
      },
      "source": [
        "從文字檔擷取 Google 文件網址清單\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "tXHmC10IitET"
      },
      "outputs": [],
      "source": [
        "url = \"https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/search/retrieval-augmented-generation/examples/URLs.txt\"\n",
        "response = requests.get(url)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    # The request was successful, and the content is in response.text\n",
        "    content = response.text\n",
        "\n",
        "URLS = [line.strip() for line in content.splitlines()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-Ly0yNVlReK"
      },
      "source": [
        "解析 HTML 和提取相關純文字區段\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "hMD6Qz_TkFMG"
      },
      "outputs": [],
      "source": [
        "# Given a Google documentation URL, retrieve a list of all text chunks within h2 sections\n",
        "def get_sections(url: str) -> list[str]:\n",
        "    page = requests.get(url)\n",
        "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
        "\n",
        "    sections = []\n",
        "    paragraphs = []\n",
        "\n",
        "    body_div = soup.find(\"div\", class_=\"devsite-article-body\")\n",
        "    for child in body_div.findChildren():\n",
        "        if child.name == \"p\":\n",
        "            paragraphs.append(child.get_text().strip())\n",
        "        if child.name == \"h2\":\n",
        "            sections.append(\" \".join(paragraphs))\n",
        "            break\n",
        "\n",
        "    for header in soup.find_all(\"h2\"):\n",
        "        paragraphs = []\n",
        "        nextNode = header.nextSibling\n",
        "        while nextNode:\n",
        "            if isinstance(nextNode, Tag):\n",
        "                if nextNode.name in {\"p\", \"ul\"}:\n",
        "                    paragraphs.append(nextNode.get_text().strip())\n",
        "                elif nextNode.name == \"h2\":\n",
        "                    sections.append(\" \".join(paragraphs))\n",
        "                    break\n",
        "            nextNode = nextNode.nextSibling\n",
        "    return sections"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "poNdlLf4kFp5"
      },
      "outputs": [],
      "source": [
        "all_text = [t for url in URLS for t in get_sections(url) if t]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wy-qw-xslYpX"
      },
      "source": [
        "注意大多數文件相對較短，但有些長達數千字\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DSkdu30tuNbY"
      },
      "outputs": [],
      "source": [
        "text_lengths = [len(t) for t in all_text]\n",
        "pd.DataFrame(text_lengths).hist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r00cIHIVlj4E"
      },
      "source": [
        "## 建立向量儲存\n",
        "\n",
        "開始初始化模型\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D26RnssLln3U"
      },
      "outputs": [],
      "source": [
        "embeddings_model = TextEmbeddingModel.from_pretrained(\"textembedding-gecko@001\")\n",
        "text_model = TextGenerationModel.from_pretrained(\"text-bison@001\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEYwgmPxlokS"
      },
      "source": [
        "為向量相似度和區塊擷取建立一些輔助函式\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SStUcSPluhvw"
      },
      "outputs": [],
      "source": [
        "# Separates seq into multiple chunks in the specified size with the specified overlap\n",
        "def split_overlap(seq, size, overlap):\n",
        "    if len(seq) <= size:\n",
        "        return [seq]\n",
        "    return [\"\".join(x) for x in zip(*[seq[i :: size - overlap] for i in range(size)])]\n",
        "\n",
        "\n",
        "# Compute the cosine similarity of two vectors, wrap as returned function to make easier to use with Pandas\n",
        "def get_similarity_fn(query_vector):\n",
        "    def fn(row):\n",
        "        return np.dot(row, query_vector) / (\n",
        "            numpy.linalg.norm(row) * numpy.linalg.norm(query_vector)\n",
        "        )\n",
        "\n",
        "    return fn\n",
        "\n",
        "\n",
        "# Retrieve embeddings from the specified model with retry logic\n",
        "@retry.Retry(timeout=300.0)\n",
        "def get_embeddings(text):\n",
        "    return embeddings_model.get_embeddings([text])[0].values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70aXFPhJmCM8"
      },
      "source": [
        "建立向量儲存，我們正在使用 Pandas DataFrame\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0cEJeeGIgFxc"
      },
      "outputs": [],
      "source": [
        "def create_vector_store(texts, chunk_size, overlap):\n",
        "    vector_store = pd.DataFrame()\n",
        "    # Insert the individual texts into the vector store\n",
        "    vector_store[\"texts\"] = list(\n",
        "        itertools.chain(*[split_overlap(t, chunk_size, overlap) for t in texts])\n",
        "    )\n",
        "\n",
        "    # Create embeddings from those texts\n",
        "    vector_store[\"embeddings\"] = (\n",
        "        vector_store[\"texts\"].progress_apply(get_embeddings).apply(np.array)\n",
        "    )\n",
        "\n",
        "    return vector_store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ifp-Y_kryXJ3"
      },
      "outputs": [],
      "source": [
        "CHUNK_SIZE = 400\n",
        "OVERLAP = 50\n",
        "\n",
        "vector_store = create_vector_store(all_text, CHUNK_SIZE, OVERLAP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ORlMIcEw0LVW"
      },
      "outputs": [],
      "source": [
        "vector_store.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAJZc3mamQli"
      },
      "source": [
        "## 搜尋向量儲存並用來生成\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdNIXBUimv01"
      },
      "source": [
        "如果我們把問題只丟給基礎模型，它會出現幻覺。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QEJKQz5ymw1f"
      },
      "outputs": [],
      "source": [
        "text_model.predict(\n",
        "    \"How long will a stable model version of text-bison be available?\"\n",
        ").text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZbX1dkAnB6V"
      },
      "source": [
        "讓我們透過從向量儲存中擷取文字來解決此問題，並告訴模型使用它們。\n",
        "\n",
        "透過嵌入查詢並搜尋相似的向量，搜尋向量儲存以插入提示中的相關文字。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "csMpD6498FXL"
      },
      "outputs": [],
      "source": [
        "def get_context(question, vector_store, num_docs):\n",
        "    # Embed the search query\n",
        "    query_vector = np.array(get_embeddings(question))\n",
        "\n",
        "    # Get similarity to all other vectors and sort, cut off at num_docs\n",
        "    top_matched = (\n",
        "        vector_store[\"embeddings\"]\n",
        "        .apply(get_similarity_fn(query_vector))\n",
        "        .sort_values(ascending=False)[:num_docs]\n",
        "        .index\n",
        "    )\n",
        "    top_matched_df = vector_store[vector_store.index.isin(top_matched)][[\"texts\"]]\n",
        "\n",
        "    # Return a string with the top matches\n",
        "    context = \" \".join(top_matched_df.texts.values)\n",
        "    return context"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6kDwMEAmnfl"
      },
      "source": [
        "建立一個包含情境和問題的提示。指示 LLM 僅使用所提供的內容來回答問題\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KfZnJF470esv"
      },
      "outputs": [],
      "source": [
        "def answer_question(question, vector_store, num_docs=10, print_prompt=False):\n",
        "    context = get_context(question, vector_store, num_docs)\n",
        "    qa_prompt = f\"\"\"Your mission is to answer questions based on a given context. Remember that before you give an answer, you must check to see if it complies with your mission.\n",
        "Context: ```{context}```\n",
        "Question: ***{question}***\n",
        "Before you give an answer, make sure it is only from information in the context. If the information is not in the context, just reply \"I don't know the answer to that\". Think step by step.\n",
        "Answer: \"\"\"\n",
        "    if print_prompt:\n",
        "        print(qa_prompt)\n",
        "    result = text_model.predict(qa_prompt, temperature=0)\n",
        "    return result.text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96kS0ZU-m6W6"
      },
      "source": [
        "觀察完全生成的提示，語境會被嵌入其中。即使輸入的語境相當淩亂，該模型現在也能做出事實的回答。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "90dMoKTr066y"
      },
      "outputs": [],
      "source": [
        "answer_question(\n",
        "    \"How long will a stable model version of text-bison be available?\",\n",
        "    vector_store,\n",
        "    print_prompt=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bmfEIvKmnmCb"
      },
      "outputs": [],
      "source": [
        "answer_question(\n",
        "    \"How long will a stable model version of text-bison be available?\", vector_store\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2A5mQ6Znvmz"
      },
      "source": [
        "## 自動化評估\n",
        "\n",
        "RAG 的這個實作依賴於段落大小、段落之間的重疊、輸入為脈絡的文字數以及提示。我們建立一個簡單的提示來評估問題的答案，這會讓我們可以調整參數並了解這些微調後的影響。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UB5wB4NR2COn"
      },
      "outputs": [],
      "source": [
        "def eval_answer(question, answer, context):\n",
        "    eval_prompt = f\"\"\"Your mission is to evaluate answers to questions based on a given context. Remember that before you give an answer, you must check to see if it complies with your mission.\n",
        "\n",
        "Context: ```{context}```\n",
        "Question: ***{question}***\n",
        "Answer: \"{answer}\"\n",
        "\n",
        "Respond only with a number from 0 to 5. Think step by step. If the provided answer is not in the context, reply 5 if it is \"I don't know the answer to that\" otherwise reply 0.\n",
        "Relevance: \"\"\"\n",
        "    # Stop sequence to cut the model off after outputting an integer\n",
        "    result = text_model.predict(\n",
        "        eval_prompt, temperature=0, max_output_tokens=1, stop_sequences=[\".\", \" \"]\n",
        "    )\n",
        "    return int(result.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVMJ9gBPoU-k"
      },
      "source": [
        "輸入數個問題並取得評估結果\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NyLMJ0u42yxY"
      },
      "outputs": [],
      "source": [
        "questions = [\n",
        "    \"What release stage is the RLHF tuning feature?\",\n",
        "    \"Can I generate hate speech with text bison?\",\n",
        "    \"What format should my batch prediction in put be in?\",\n",
        "    \"How can I get the number of tokens?\",\n",
        "    \"How do I create a custom style model?\",\n",
        "    \"What is the dimensionality of the vector created by the multimodal model?\",\n",
        "    \"How long will a stable model verison be available?\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BftOPiMKFm_8"
      },
      "outputs": [],
      "source": [
        "answers = [answer_question(q, vector_store) for q in questions]\n",
        "contexts = [get_context(q, vector_store, 10) for q in questions]\n",
        "idks = [\"I don't know\" in a for a in answers]\n",
        "evals = [\n",
        "    (question, answer, context, eval_answer(question, answer, context), idk)\n",
        "    for question, answer, context, idk in zip(questions, answers, contexts, idks)\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zb7VfarNF9W1"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(evals, columns=[\"question\", \"answer\", \"context\", \"score\", \"idk\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_X2OjzsodzI"
      },
      "source": [
        "現在調整參數，看看效能有什麼差異\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tWKZent6MNf4"
      },
      "outputs": [],
      "source": [
        "def eval_on_params(chunk_size, overlap, num_docs):\n",
        "    vector_store = create_vector_store(all_text, chunk_size, overlap)\n",
        "    answers = [answer_question(q, vector_store) for q in questions]\n",
        "    contexts = [get_context(q, vector_store, num_docs) for q in questions]\n",
        "    idks = [\"I don't know\" in a for a in answers]\n",
        "    evals = [\n",
        "        (question, answer, context, eval_answer(question, answer, context), idk)\n",
        "        for question, answer, context, idk in zip(questions, answers, contexts, idks)\n",
        "    ]\n",
        "    return pd.DataFrame(\n",
        "        evals, columns=[\"question\", \"answer\", \"context\", \"score\", \"idk\"]\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92rq4ZGerqNX"
      },
      "source": [
        "較小的 chunk 大小需要較長的時間來產生嵌入向量\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zuweYEbVgoZt"
      },
      "outputs": [],
      "source": [
        "smaller_context_df = eval_on_params(100, 0, 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J38F4YZpi1Bf"
      },
      "outputs": [],
      "source": [
        "smaller_context_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQC7gWPWokJO"
      },
      "source": [
        "較大的背景大小已建立了更多未知數。當 LLM 組合到系統中時，請仔細考慮如何衡量系統中各個組成的效能。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4jmWGmzdgwUI"
      },
      "outputs": [],
      "source": [
        "larger_context_df = eval_on_params(1000, 200, 15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BNOwnFQwizBb"
      },
      "outputs": [],
      "source": [
        "larger_context_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xxVrXRR5jRU_"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}