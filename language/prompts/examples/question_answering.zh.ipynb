{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ur8xi4C7S06n"
      },
      "outputs": [],
      "source": [
        "# Copyright 2023 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAPoU8Sm5E6e"
      },
      "source": [
        "# 用 Vertex AI 上的生成式模型問答\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/doggy8088/generative-ai/blob/main/language/prompts/examples/question_answering.zh.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Google Colaboratory logo\"><br> 於 Colab 中執行\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/doggy8088/generative-ai/blob/main/language/prompts/examples/question_answering.zh.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\"><br> 在 GitHub 上檢視\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/doggy8088/generative-ai/blob/main/language/prompts/examples/question_answering.zh.ipynb\">\n",
        "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br> 於 Vertex AI Workbench 中開啟\n",
        "    </a>\n",
        "  </td>\n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "| | |\n",
        "|-|-|\n",
        "|作者 | [Polong Lin](https://github.com/polong-lin) |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvgnzT1CKxrO"
      },
      "source": [
        "## 概觀\n",
        "\n",
        "大型語言模型可用於各種自然語言處理任務，包括問答 (Q&A)。這些模型是建構在大量的文本資料上進行訓練，並且可以對各種問題產生高品質的回應。此處需要注意的一件事是，大多數模型都設定了知識截止日期，因此提出任何太新的問題可能會導致不完整、想像的或是不正確的答案 (即幻覺)。\n",
        "\n",
        "這個筆記本涵蓋了使用生成式模型回答問題的提示基本要素。此外，它展示了「開放域」(網際網路上公開的知識) 和「封閉域」(更私密的知識——通常為企業或個人知識)。\n",
        "\n",
        "在 [官方文件](https://cloud.google.com/vertex-ai/docs/generative-ai/text/text-overview#prompt_structure) 中進一步瞭解提示設計。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d975e698c9a4"
      },
      "source": [
        "### 目標\n",
        "\n",
        "在筆記本的結尾，你應該能夠針對下列內容撰寫提示：\n",
        "\n",
        "* **開放領域** 問題：\n",
        "    * 零次學習提示\n",
        "    * 少次學習提示\n",
        "\n",
        "\n",
        "* **封閉領域** 問題：\n",
        "    * 提供客製知識作為背景\n",
        "    * 指令微調輸出\n",
        "    * 少次學習提示\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDU0XJ1xRDlL"
      },
      "source": [
        "## 開始使用\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2a5AEr0lkLKD"
      },
      "source": [
        "### 安裝 Vertex AI SDK\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82ad0c445061"
      },
      "outputs": [],
      "source": [
        "!pip install google-cloud-aiplatform --upgrade --user"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**僅 Colab：** 取消下一個Cell註解以重新啟動Kernel或使用按鈕重新啟動Kernel。對於 Vertex AI Workbench，你可以使用頂端的按鈕重新啟動終端機。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Hsqwn4hkLKE"
      },
      "outputs": [],
      "source": [
        "# # Automatically restart kernel after installs so that your environment can access the new packages\n",
        "# import IPython\n",
        "\n",
        "# app = IPython.Application.instance()\n",
        "# app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xe7OuYuGkLKF"
      },
      "source": [
        "### 驗證筆記本環境\n",
        "* 如果你使用 **Colab** 執行此筆記本，取消註解下方的Cell並繼續。\n",
        "* 如果你使用 **Vertex AI 工作台** ，請查看[此處](https://github.com/doggy8088/generative-ai/tree/main/setup-env)的設定說明。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U9Gx2SAZkLKF"
      },
      "outputs": [],
      "source": [
        "# from google.colab import auth\n",
        "# auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "960505627ddf"
      },
      "source": [
        "### 匯入函式庫\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**僅限 Colab：** 取消下方單元格的註解，以初始化 Vertex AI SDK。對於 Vertex AI Workbench，不需要執行此動作。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import vertexai\n",
        "\n",
        "# PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\n",
        "# vertexai.init(project=PROJECT_ID, location=\"us-central1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PyQmSRbKA8r-"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from vertexai.language_models import TextGenerationModel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UP76a2la7O-a"
      },
      "source": [
        "### 載入模型\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7isig7e07O-a"
      },
      "outputs": [],
      "source": [
        "generation_model = TextGenerationModel.from_pretrained(\"text-bison@001\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIPcn5dZ7O-b"
      },
      "source": [
        "## 問題解答\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNNEz7vGFYUP"
      },
      "source": [
        "問答功能需要提供提示或問題，模型可以藉此產生回應。提示可以是幾個字或幾個完整的句子，視問題的複雜程度而定。\n",
        "\n",
        "在建立問答提示時，重要的是要具體且提供盡可能多的背景。這有助於模型了解問題背後的意圖並產生相關的回應。例如，如果你想問：\n",
        "\n",
        "```\n",
        "「法國的首都是什麼？」\n",
        "\n",
        "那麼一個很好的提示可能是：\n",
        "\n",
        "「請告訴我哪個城市是法國的首都。」\n",
        "\n",
        "```\n",
        "\n",
        "除了具體之外，提示也應該是語法正確且沒有拼寫錯誤。這有助於模型產生容易理解、錯誤或不準確性較少的回應。\n",
        "\n",
        "透過提供具體且豐富背景的提示，你可以協助模型了解問題背後的意圖，並產生準確且相關的回應。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5N9ZnlECm-z"
      },
      "source": [
        "以下是問答提示問題的**開放領域** 和**封閉領域** 類別之間的一些差異。\n",
        "\n",
        "＊**開放領域** ：所有答案已在線上獲得的提問。它們可以屬於任何類別，例如歷史、地理、國家、政治、化學等。這些包括瑣事或常識問題，例如：\n",
        "\n",
        "```\n",
        "問：誰在奧運遊泳項目奪得金牌？\n",
        "問：[特定國家]的總統是誰？\n",
        "問：誰寫了 [具體書名]？\n",
        "```\n",
        "\n",
        "請記住生成式模型的訓練截點，因為涉及模型訓練後比最新資訊的問題可能會給出不正確或天馬行空的答案。\n",
        "\n",
        "＊**封閉領域** ：如果你有一些不在網際網路供應的內部知識基礎，那麼它們屬於_封閉領域_類別。\n",
        "你可以將「私人」知識作為脈絡傳遞給模型。如果提示正確，模型更有可能在提供的脈絡中回答，而不太可能超越網路提供範圍以外的答案。\n",
        "\n",
        "考慮在你的產品內部文件上建立問答機器人的範例。在這種情況下，你可以將完整的文件傳遞給模型，並僅提示它根據文件回答問題。\n",
        "\n",
        "**封閉領域** 的典型提示：\n",
        "\n",
        "```\n",
        "提示：f\"\"\" 從以下脈絡中回答： \\n\\n\n",
        "     脈絡：{{你的知識庫}} \\n\n",
        "     問題：{{該知識庫的具體問題}} \\n\n",
        "     回答：{{由模型預測}} \\n\n",
        "\"\"\"\n",
        "```\n",
        "\n",
        "以下是理解這些不同類型的提示的一些範例。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBoN6zixDSiX"
      },
      "source": [
        "### 開放領域\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJnv8XhnDXQm"
      },
      "source": [
        "#### 零次提示\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PaYoQuRwCm-z"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"Q: Who was President of the United States in 1955? Which party did he belong to?\\n\n",
        "            A:\n",
        "         \"\"\"\n",
        "print(\n",
        "    generation_model.predict(\n",
        "        prompt,\n",
        "        max_output_tokens=256,\n",
        "        temperature=0.1,\n",
        "    ).text\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4qcUdUgwCm-z"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"Q: What is the tallest mountain in the world?\\n\n",
        "            A:\n",
        "         \"\"\"\n",
        "print(\n",
        "    generation_model.predict(\n",
        "        prompt,\n",
        "        max_output_tokens=20,\n",
        "        temperature=0.1,\n",
        "    ).text\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HShw52X2Dcmx"
      },
      "source": [
        "#### 少樣本提示\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tj_2hHAWE8vh"
      },
      "source": [
        "假設你想從模型中獲得簡短答案 (例如，只有一個特定名稱)。為了解決此問題，你可以利用少數提示來提供範例給模型，以說明預期的行為。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RE5yCAaqDg7m"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"Q: Who is the current President of France?\\n\n",
        "            A: Emmanuel Macron \\n\\n\n",
        "\n",
        "            Q: Who invented the telephone? \\n\n",
        "            A: Alexander Graham Bell \\n\\n\n",
        "\n",
        "            Q: Who wrote the novel \"1984\"?\n",
        "            A: George Orwell\n",
        "\n",
        "            Q: Who discovered penicillin?\n",
        "            A:\n",
        "         \"\"\"\n",
        "print(\n",
        "    generation_model.predict(\n",
        "        prompt,\n",
        "        max_output_tokens=20,\n",
        "        temperature=0.1,\n",
        "    ).text\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGvs0jFsUlvM"
      },
      "source": [
        "#### 零次提示 vs 少次提示\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yjsAMuMUfZC"
      },
      "source": [
        "零次提示對於快速為新任務產生文字是有用的，但產生文字的品質可能會比具有精挑細選範例的少量提示還低。少量提示通常更適合需要高度特定性或領域特定知識的任務，但需要一些額外的思考，並可能需要資料來設定提示。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6UiJTxXEs4t"
      },
      "source": [
        "### 封閉領域\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03ZITm4AGBvP"
      },
      "source": [
        "#### 將內部知識當作提示文中的內容\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkhqjmB6VqPx"
      },
      "source": [
        "想像一種情境：你想建立一個問答機器人，可以擷取內部文件，並讓使用者可以對其提出問題。\n",
        "\n",
        "在以下範例中，Google Cloud Storage 和內容政策文件會新增到提示中，讓 PaLM API 可以使用其，在提供的脈絡下回答後續問題。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s1H2er_lExpW"
      },
      "outputs": [],
      "source": [
        "context = \"\"\"\n",
        "Storage and content policy \\n\n",
        "How durable is my data in Cloud Storage? \\n\n",
        "Cloud Storage is designed for 99.999999999% (11 9's) annual durability, which is appropriate for even primary storage and\n",
        "business-critical applications. This high durability level is achieved through erasure coding that stores data pieces redundantly\n",
        "across multiple devices located in multiple availability zones.\n",
        "Objects written to Cloud Storage must be redundantly stored in at least two different availability zones before the\n",
        "write is acknowledged as successful. Checksums are stored and regularly revalidated to proactively verify that the data\n",
        "integrity of all data at rest as well as to detect corruption of data in transit. If required, corrections are automatically\n",
        "made using redundant data. Customers can optionally enable object versioning to add protection against accidental deletion.\n",
        "\"\"\"\n",
        "\n",
        "question = \"How is high availability achieved?\"\n",
        "\n",
        "prompt = f\"\"\"Answer the question given in the contex below:\n",
        "Context: {context}?\\n\n",
        "Question: {question} \\n\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "print(\"[Prompt]\")\n",
        "print(prompt)\n",
        "\n",
        "print(\"[Response]\")\n",
        "print(\n",
        "    generation_model.predict(\n",
        "        prompt,\n",
        "    ).text\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tagWC4VcQIw6"
      },
      "source": [
        "#### Instruction-tuning 的輸出\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9UkogWHXM6N"
      },
      "source": [
        "另一種協助語言模型的方式，是在提示中提供額外的指示來建構輸出。為了確保模型不會回應脈絡之外的任何內容，該提示可以指定，如果情況如此，回應應為「所提供脈絡中沒有的資訊。」\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ouq8FfwSQIBT"
      },
      "outputs": [],
      "source": [
        "question = \"What machined are required for hosting Vertex AI models?\"\n",
        "prompt = f\"\"\"Answer the question given the context below as {{Context:}}. \\n\n",
        "If the answer is not available in the {{Context:}} and you are not confident about the output,\n",
        "please say \"Information not available in provided context\". \\n\\n\n",
        "Context: {context}?\\n\n",
        "Question: {question} \\n\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "print(\"[Prompt]\")\n",
        "print(prompt)\n",
        "\n",
        "print(\"[Response]\")\n",
        "print(\n",
        "    generation_model.predict(\n",
        "        prompt,\n",
        "        max_output_tokens=256,\n",
        "        temperature=0.3,\n",
        "    ).text\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZJfZShPRGqU"
      },
      "source": [
        "#### 少樣本提示\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qdSEQeQIS6pt"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"\n",
        "Context:\n",
        "The term \"artificial intelligence\" was first coined by John McCarthy in 1956. Since then, AI has developed into a vast\n",
        "field with numerous applications, ranging from self-driving cars to virtual assistants like Siri and Alexa.\n",
        "\n",
        "Question:\n",
        "What is artificial intelligence?\n",
        "\n",
        "Answer:\n",
        "Artificial intelligence refers to the simulation of human intelligence in machines that are programmed to think and learn like humans.\n",
        "\n",
        "---\n",
        "\n",
        "Context:\n",
        "The Wright brothers, Orville and Wilbur, were two American aviation pioneers who are credited with inventing and\n",
        "building the world's first successful airplane and making the first controlled, powered and sustained heavier-than-air human flight,\n",
        " on December 17, 1903.\n",
        "\n",
        "Question:\n",
        "Who were the Wright brothers?\n",
        "\n",
        "Answer:\n",
        "The Wright brothers were American aviation pioneers who invented and built the world's first successful airplane\n",
        "and made the first controlled, powered and sustained heavier-than-air human flight, on December 17, 1903.\n",
        "\n",
        "---\n",
        "\n",
        "Context:\n",
        "The Mona Lisa is a 16th-century portrait painted by Leonardo da Vinci during the Italian Renaissance. It is one of\n",
        "the most famous paintings in the world, known for the enigmatic smile of the woman depicted in the painting.\n",
        "\n",
        "Question:\n",
        "Who painted the Mona Lisa?\n",
        "\n",
        "Answer:\n",
        "\n",
        "\"\"\"\n",
        "print(\n",
        "    generation_model.predict(\n",
        "        prompt,\n",
        "    ).text\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "leYIui80Q4tH"
      },
      "source": [
        "### 萃取式問答\n",
        "\n",
        "在以下範例中，生成模型會被引導去理解問題和文章的用意，並在文章中找出相關資訊來回答問題。模型會收到一個問題和一段文字，並被要求在文字中找出問題的答案。答案通常是一短語或句子。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bPZqm0QJQ4tH"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"\n",
        "Background: There is evidence that there have been significant changes in Amazon rainforest vegetation over the last 21,000 years through the Last Glacial Maximum (LGM) and subsequent deglaciation.\n",
        "Analyses of sediment deposits from Amazon basin paleo lakes and from the Amazon Fan indicate that rainfall in the basin during the LGM was lower than for the present, and this was almost certainly\n",
        "associated with reduced moist tropical vegetation cover in the basin. There is debate, however, over how extensive this reduction was. Some scientists argue that the rainforest was reduced to small,\n",
        "isolated refugia separated by open forest and grassland; other scientists argue that the rainforest remained largely intact but extended less far to the north, south, and east than is seen today.\n",
        "This debate has proved difficult to resolve because the practical limitations of working in the rainforest mean that data sampling is biased away from the center of the Amazon basin, and both\n",
        "explanations are reasonably well supported by the available data.\n",
        "\n",
        "Q: What does LGM stands for?\n",
        "A: Last Glacial Maximum.\n",
        "\n",
        "Q: What did the analysis from the sediment deposits indicate?\n",
        "A: Rainfall in the basin during the LGM was lower than for the present.\n",
        "\n",
        "Q: What are some of scientists arguments?\n",
        "A: The rainforest was reduced to small, isolated refugia separated by open forest and grassland.\n",
        "\n",
        "Q: There have been major changes in Amazon rainforest vegetation over the last how many years?\n",
        "A: 21,000.\n",
        "\n",
        "Q: What caused changes in the Amazon rainforest vegetation?\n",
        "A: The Last Glacial Maximum (LGM) and subsequent deglaciation\n",
        "\n",
        "Q: What has been analyzed to compare Amazon rainfall in the past and present?\n",
        "A: Sediment deposits.\n",
        "\n",
        "Q: What has the lower rainfall in the Amazon during the LGM been attributed to?\n",
        "A:\n",
        "\"\"\"\n",
        "\n",
        "print(\n",
        "    generation_model.predict(\n",
        "        prompt,\n",
        "    ).text\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94d80fb55f48"
      },
      "source": [
        "### 評估\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b620d23a7634"
      },
      "source": [
        "如果每個問題的實際答案可用，你可以評估問答任務的輸出結果。在零次提示中，你只能使用「開放域」問題。不過，對於「封閉域」問題，你可以加入脈絡並以類似方式評估。要展示其運作方式，首先使用問題和實際答案建立一個簡單的資料框。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8e813a463531"
      },
      "outputs": [],
      "source": [
        "qa_data = {\n",
        "    \"question\": [\n",
        "        \"In a website browser address bar, what does “www” stand for?\",\n",
        "        \"Who was the first woman to win a Nobel Prize\",\n",
        "        \"What is the name of the Earth’s largest ocean?\",\n",
        "    ],\n",
        "    \"answer_groundtruth\": [\"World Wide Web\", \"Marie Curie\", \"The Pacific Ocean\"],\n",
        "}\n",
        "qa_data_df = pd.DataFrame(qa_data)\n",
        "qa_data_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "951a147dc79d"
      },
      "source": [
        "現在，既然你擁有附帶問題與正解答案的資料，可以使用 `apply` 函式對每個檢閱列呼叫 PaLM 2 生成模型。每列會使用動態提示使用 PaLM API 預測答案。我們會將結果儲存在 `answer_prediction` 欄位。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ffc47e0cb5b9"
      },
      "outputs": [],
      "source": [
        "def get_answer(row):\n",
        "    prompt = f\"\"\"Answer the following question as precise as possible.\\n\\n\n",
        "            question: {row}\n",
        "            answer:\n",
        "              \"\"\"\n",
        "    return generation_model.predict(\n",
        "        prompt=prompt,\n",
        "    ).text\n",
        "\n",
        "\n",
        "qa_data_df[\"answer_prediction\"] = qa_data_df[\"question\"].apply(get_answer)\n",
        "qa_data_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fe997dbf788"
      },
      "source": [
        "你可能想評估 PaLM API 預測的答案。然而，它會比文字分類複雜，因為答案可能與真實情況不同，並且可能以略多或略少的字詞呈現。\n",
        "\n",
        "例如，你可以觀察問題「地球上最大的海洋是什麼？」並看到模型在真實情況標籤為「太平洋」時預測為「Pacific Ocean」，多了一個「The」。現在，如果你使用簡單的分類指標，你會認為這是一個錯誤的預測，因為原始字串和預測字串有所不同。但是，你可以看到答案是正確的，因為多了一個「The」造成了問題。這是簡單的字串比較問題。\n",
        "\n",
        "關於字串比較，其中 `ground_thruth` 和 `predicted` 可能有一些額外的或較少的字母，一種方法是使用模糊匹配演算法。\n",
        "\n",
        "模糊字串匹配使用 [Levenshtein 距離](https://zh.wikipedia.org/wiki/Levenshtein%E8%B7%9D%E7%A6%BB) 來計算兩者之間的差異字串。\n",
        "\n",
        "例如，\"kitten\" 和 \"sitting\" 之間的 Levenshtein 距離為 3，因為以下 3 個編輯將一個字詞轉換另一個字詞，並且無法使用小於 3 個編輯來進行轉換：\n",
        "\n",
        "* kitten → sitten (將 \"k\" 替換為 \"s\")，\n",
        "* sitten → sittin (將 \"e\" 替換為 \"i\")，\n",
        "* sittin → sitting (在結尾插入 \"g\")。\n",
        "\n",
        "\n",
        "以下是另一個範例，但這次使用 `fuzzywuzzy` 函式庫，它在兩個字串之間給予相同的 `Levenshtein 距離`，但以比率方式。比率原始分數將字串的相似性視為範圍 [0, 100] 內的整數。對於兩個字串 X 和 Y，分數定義為 int(round((2.0 * M / T) * 100))，其中 T 是兩個字串中字元的總數，而 M 是兩個字串中匹配的字元數。\n",
        "\n",
        "在此處進一步了解有關 [比率公式](https://anhaidgroup.github.io/py_stringmatching/v0.3.x/Ratio.html) 的資訊：\n",
        "\n",
        "你可以參閱一個範例以進一步瞭解這一點。\n",
        "```\n",
        "字串 1：「這是一個測試」\n",
        "字串 2：「這是一個測試！」\n",
        "\n",
        "模糊比率 => 97  #\n",
        "\n",
        "模糊部分比率 => 100  # 由於大多數字元相同且序列相似，因此演算法將部分比率計算為 100，並忽略簡單的增減 (新字元)。\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7b170579a455"
      },
      "source": [
        "首先，安裝 package `fuzzywuzzy` 和 `python-Levenshtein`：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6c55ea0eaed0"
      },
      "outputs": [],
      "source": [
        "!pip install -q python-Levenshtein --upgrade --user\n",
        "!pip install -q fuzzywuzzy --upgrade --user"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5f048152519f"
      },
      "source": [
        "然後計算一個分數來執行模糊匹配：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "040c1f9a175b"
      },
      "outputs": [],
      "source": [
        "from fuzzywuzzy import fuzz\n",
        "\n",
        "\n",
        "def get_fuzzy_match(df):\n",
        "    return fuzz.partial_ratio(df[\"answer_groundtruth\"], df[\"answer_prediction\"])\n",
        "\n",
        "\n",
        "qa_data_df[\"match_score\"] = qa_data_df.apply(get_fuzzy_match, axis=1)\n",
        "qa_data_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11e266c49860"
      },
      "source": [
        "現在你已取得個別匹配評分 (部分)，便可計算整欄的平均值，以了解整體數據概況。\n",
        "接近 100 的分數表示 PaLM 2 可更接近實際預測；如果分數接近 50 或 0，表示未有良好表現。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dae6a92a7650"
      },
      "outputs": [],
      "source": [
        "print(\n",
        "    \"the average match score of all predicted answer from PaLM 2 is : \",\n",
        "    qa_data_df[\"match_score\"].mean(),\n",
        "    \" %\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9e78972cad1"
      },
      "source": [
        "在這種情況下，即使有些預測遺失了一些字詞，你的平均分數也會得到 100%。這表示你非常接近真實結果，有些答案只是少了真實結果的明確冗長說明。\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "question_answering.ipynb",
      "toc_visible": true
    },
    "environment": {
      "kernel": "python3",
      "name": "tf2-gpu.2-11.m108",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-11:m108"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}