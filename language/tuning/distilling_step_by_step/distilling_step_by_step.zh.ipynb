{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3710d312-cbbe-4fc1-9db6-712e8bccddd9",
      "metadata": {
        "id": "3710d312-cbbe-4fc1-9db6-712e8bccddd9"
      },
      "source": [
        "# 蒸餾步驟教學\n",
        "### 分步指南\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/doggy8088/generative-ai/blob/main/language/tuning/distilling_step_by_step/distilling_step_by_step.zh.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Google Colaboratory 標誌\"><br> 在 Colab 中執行\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/doggy8088/generative-ai/blob/main/language/tuning/distilling_step_by_step/distilling_step_by_step.zh.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub 標誌\"><br> 在 GitHub 上檢視\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/doggy8088/generative-ai/blob/main/language/tuning/distilling_step_by_step/distilling_step_by_step.zh.ipynb\">\n",
        "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI 標誌\"><br> 在 Vertex AI Workbench 中開啟\n",
        "    </a>\n",
        "  </td>\n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "763bc5ee",
      "metadata": {},
      "source": [
        "| | |\n",
        "|-|-|\n",
        "|作者 | [Anirudh Haritas Murali](https://github.com/anihm136) |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7be8b8d-6e73-427f-a420-4e23a0001ad8",
      "metadata": {
        "id": "a7be8b8d-6e73-427f-a420-4e23a0001ad8"
      },
      "source": [
        "# 概述\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e14832e-4aab-4051-b37f-1718db87c252",
      "metadata": {
        "id": "3e14832e-4aab-4051-b37f-1718db87c252"
      },
      "source": [
        "**蒸餾** 是機器學習中的一種技術，允許我們提取大型模型的學習成果，並使用較小的模型來表示。這允許改善可擴充性，因為較小的模型執行時需要較少的資源，並且生成推論所需的時間較少，同時仍能達到接近較大模型的準確度。\n",
        "\n",
        "傳統上，蒸餾使用較大模型的內部參數 (特別是對數) 來訓練較小的模型。然而，當今表現最好的大型語言模型之一，包括 Google 的 [PaLM 2](https://ai.google/discover/palm2/) 模型，都以 API 形式提供給消費者，而沒有辦法訪問內部參數。直到最近，這才禁止使用這些模型作為蒸餾的教師模型。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c72c65c-b6b5-47ac-915b-f1a19c5cf312",
      "metadata": {
        "id": "3c72c65c-b6b5-47ac-915b-f1a19c5cf312"
      },
      "source": [
        "## 目標\n",
        "\n",
        "在本筆記本中，我們將檢閱論文 [逐步萃取](https://blog.research.google/2023/09/distilling-step-by-step-outperforming.html) 中描述的技術，其中說明了一種將大型 LLM 的知識萃取到較小型 LLM 中的新方法，而且不需要大型模型的內部參數。此研究的原始碼可在 [https://github.com/google-research/distilling-step-by-step](https://github.com/google-research/distilling-step-by-step) 取得。\n",
        "\n",
        "我們將逐步訓練一個小型 (學生) 模型，以模擬較大型 (教師) 模型的推理能力。透過訓練學生模型模擬推理能力，而非實際輸出，我們可以讓小型模型更好地概化到其他未見的輸入。\n",
        "\n",
        "所執行的步驟包括：\n",
        "\n",
        "- 為萃取準備一個資料集\n",
        "- 設定萃取管道\n",
        "- 使用 PaLM 作為教師模型來訓練學生模型\n",
        "- 評估萃取模型的效能\n",
        "- 將萃取模型部署到 Vertex AI\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7d4286f-c81f-4b5d-8bed-b2eb1daf550c",
      "metadata": {
        "id": "d7d4286f-c81f-4b5d-8bed-b2eb1daf550c"
      },
      "source": [
        "## 成本\n",
        "本教學課程使用了 Google Cloud 的計費元件：\n",
        "- Vertex AI\n",
        "- 儲存空間\n",
        "- Artifact Registry\n",
        "- Cloud Build\n",
        "\n",
        "了解 [Vertex AI 價格](https://cloud.google.com/vertex-ai/pricing)、[儲存空間價格](https://cloud.google.com/storage/pricing)、[Artifact Registry 價格](https://cloud.google.com/artifact-registry/pricing) 和 [Cloud Build](https://cloud.google.com/build/pricing) 的價格，並使用 [價格計算器](https://cloud.google.com/products/calculator/) 根據預期的用量產生成本估計。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b88b40ab-9b54-4435-aa50-5746381cd4e7",
      "metadata": {
        "id": "b88b40ab-9b54-4435-aa50-5746381cd4e7"
      },
      "source": [
        "# 入門\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xW8juKok2cJF",
      "metadata": {
        "id": "xW8juKok2cJF"
      },
      "source": [
        "## (僅在 Colab) 以使用者的身分驗證\n",
        "在 Colab 上，我們將驗證具有權存取上述 Google Cloud 資源的使用者。當我們部署模型時，將會需要這項權限\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HAeoGdSR2q5m",
      "metadata": {
        "id": "HAeoGdSR2q5m"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8xk5H9iF04xH",
      "metadata": {
        "id": "8xk5H9iF04xH"
      },
      "source": [
        "## 下載支援檔案\n",
        "為簡化執行此示範的程序，已提供一些支援檔案 (資料集的 PaLM 輸出和建立模型服務容器的程式碼）\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4rgWlsRg05Nh",
      "metadata": {
        "id": "4rgWlsRg05Nh"
      },
      "outputs": [],
      "source": [
        "! gsutil -m cp -r gs://github-repo/distillation/* .\n",
        "! wget https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/language/tuning/distilling_step_by_step/requirements.txt\n",
        "! wget https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/language/tuning/distilling_step_by_step/prediction_container/Dockerfile -P prediction_container\n",
        "! wget https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/language/tuning/distilling_step_by_step/prediction_container/app/main.py -P prediction_container/app\n",
        "! wget https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/language/tuning/distilling_step_by_step/prediction_container/app/requirements.txt -P prediction_container/app\n",
        "! wget https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/language/tuning/distilling_step_by_step/prediction_container/app/requirements-torch.txt -P prediction_container/app\n",
        "! wget https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/language/tuning/distilling_step_by_step/prediction_container/app/prestart.sh -P prediction_container/app"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8ce302a-67c3-41b4-80ed-4d48d783170a",
      "metadata": {
        "id": "c8ce302a-67c3-41b4-80ed-4d48d783170a"
      },
      "source": [
        "## 安裝必要的函式庫\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "306ce182-e072-4576-9c6c-08652b3f1e16",
      "metadata": {
        "id": "306ce182-e072-4576-9c6c-08652b3f1e16"
      },
      "outputs": [],
      "source": [
        "! pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32651b1a-d005-478b-8c8a-adc58b30c6e5",
      "metadata": {
        "id": "32651b1a-d005-478b-8c8a-adc58b30c6e5"
      },
      "source": [
        "## 啟用所需的 Google Cloud API\n",
        "為了便於清理資源，你可以在本教學課程的末尾建立一個新專案並將其刪除\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5ee8dab-5f96-4d46-a29c-1d8ac6ed825e",
      "metadata": {
        "id": "e5ee8dab-5f96-4d46-a29c-1d8ac6ed825e"
      },
      "outputs": [],
      "source": [
        "PROJECT = \"\"  # @param {type:\"string\"}\n",
        "REGION = \"us-central1\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b62c1014-5024-4407-8c11-4d4aa278141b",
      "metadata": {
        "id": "b62c1014-5024-4407-8c11-4d4aa278141b"
      },
      "outputs": [],
      "source": [
        "!gcloud services enable aiplatform.googleapis.com --project {PROJECT}\n",
        "!gcloud services enable artifactregistry.googleapis.com --project {PROJECT}\n",
        "!gcloud services enable cloudbuild.googleapis.com --project {PROJECT}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f998a431-4219-4716-a61a-b7ebe48cace5",
      "metadata": {
        "id": "f998a431-4219-4716-a61a-b7ebe48cace5"
      },
      "source": [
        "# 步驟 1：資料準備\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aff2eb53-9454-46ac-b3a4-5b01e93ba78e",
      "metadata": {
        "id": "aff2eb53-9454-46ac-b3a4-5b01e93ba78e"
      },
      "source": [
        "我們的數據集將需要三個欄位 -\n",
        "1. LLM 的輸入提示\n",
        "2. 基本事實標籤，這是預期的輸出\n",
        "3. 「理由」，這是老師模型 (使用 CoT 提示) 產生的推理\n",
        "\n",
        "在此，我們將使用 HuggingFace 的 [常識解釋](https://huggingface.co/datasets/cos_e) 數據集來訓練我們的學生模型。此數據集包含約 10k 個訓練範例和 1.2k 個測試範例。我們將使用預先生成自 PaLM 模型的理由作為老師，我們將預處理數據集以符合上述結構\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ebefb0de-17b8-4f41-b56a-f3afdf21e8c3",
      "metadata": {
        "id": "ebefb0de-17b8-4f41-b56a-f3afdf21e8c3"
      },
      "outputs": [],
      "source": [
        "from datasets import DatasetDict, load_dataset\n",
        "from typing import Dict, Any, List"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfe43325-f2c7-4627-a561-1fe7aeb7828f",
      "metadata": {
        "id": "dfe43325-f2c7-4627-a561-1fe7aeb7828f"
      },
      "outputs": [],
      "source": [
        "SOURCE_DATASET = \"cos_e\"  # @param {type:\"string\"}\n",
        "SOURCE_DATASET_VERSION = \"v1.11\"  # @param {type:\"string\"}\n",
        "\n",
        "dataset = load_dataset(SOURCE_DATASET, SOURCE_DATASET_VERSION)\n",
        "dataset[\"test\"] = dataset[\"validation\"]\n",
        "del dataset[\"validation\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52a8dd69-70db-48bb-a421-71436ccd2ea9",
      "metadata": {
        "id": "52a8dd69-70db-48bb-a421-71436ccd2ea9"
      },
      "outputs": [],
      "source": [
        "def prepare_input(example: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    question = example[\"question\"]\n",
        "    c_0 = example[\"choices\"][0]\n",
        "    c_1 = example[\"choices\"][1]\n",
        "    c_2 = example[\"choices\"][2]\n",
        "    c_3 = example[\"choices\"][3]\n",
        "    c_4 = example[\"choices\"][4]\n",
        "\n",
        "    input = f\"{question}\\nAnswer Choices:\\n(a) {c_0}\\n(b) {c_1}\\n(c) {c_2}\\n(d) {c_3}\\n(e) {c_4}\"\n",
        "\n",
        "    example[\"input\"] = input\n",
        "    example[\"label\"] = example[\"answer\"]\n",
        "\n",
        "    return example\n",
        "\n",
        "\n",
        "dataset = dataset.map(\n",
        "    prepare_input,\n",
        "    remove_columns=[\n",
        "        \"id\",\n",
        "        \"question\",\n",
        "        \"choices\",\n",
        "        \"answer\",\n",
        "        \"abstractive_explanation\",\n",
        "        \"extractive_explanation\",\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81c110ef-384f-4b8e-a58a-8c81e3ad6d26",
      "metadata": {
        "id": "81c110ef-384f-4b8e-a58a-8c81e3ad6d26"
      },
      "outputs": [],
      "source": [
        "LLM_OUTPUTS_FILE_PREFIX = \"PaLM_CoT\"  # @param {type:\"string\"}\n",
        "LLM_OUTPUTS_FILE = LLM_OUTPUTS_FILE_PREFIX + \"_{split}.json\"\n",
        "\n",
        "\n",
        "def add_llm_outputs(dataset: DatasetDict, split: str) -> None:\n",
        "    llm_ds = load_dataset(\"json\", data_files=LLM_OUTPUTS_FILE.format(split=split))[\n",
        "        \"train\"\n",
        "    ]\n",
        "\n",
        "    def _add(example: Dict[str, Any], idx: int) -> Dict[str, Any]:\n",
        "        example[\"llm_rationale\"] = llm_ds[idx][\"rationale\"]\n",
        "        example[\"llm_label\"] = llm_ds[idx][\"label\"]\n",
        "        return example\n",
        "\n",
        "    dataset[split] = dataset[split].map(_add, with_indices=True)\n",
        "\n",
        "\n",
        "for split in [\"train\", \"test\"]:\n",
        "    add_llm_outputs(dataset, split)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e80e8263-1c01-4687-ade0-e85a194e17f6",
      "metadata": {
        "id": "e80e8263-1c01-4687-ade0-e85a194e17f6"
      },
      "source": [
        "# 步驟 2：建立模型\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc9306cc-8db2-4715-9ced-8a72d5e47fad",
      "metadata": {
        "id": "cc9306cc-8db2-4715-9ced-8a72d5e47fad"
      },
      "outputs": [],
      "source": [
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    DataCollatorForSeq2Seq,\n",
        "    Seq2SeqTrainer,\n",
        ")\n",
        "import pandas as pd\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "019a6709-25ba-4036-9735-47c8a105abf6",
      "metadata": {
        "id": "019a6709-25ba-4036-9735-47c8a105abf6"
      },
      "source": [
        "這裡我們將使用 T5 模型作為預訓練基礎進行蒸餾，且我們將使用對應的 tokenizer。你可以將模型名稱換成 HuggingFace Hub 上其他模型名稱、使用不同的預訓練模型 (及其 tokenizer)，或對你自己的資料集從頭訓練自訂模型/tokenizer。請注意，你需要更多資料量和運算資源才能從頭開始訓練一個良好的模型。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5da620fa-5e76-47b5-b29f-08eaa239ad21",
      "metadata": {
        "id": "5da620fa-5e76-47b5-b29f-08eaa239ad21"
      },
      "outputs": [],
      "source": [
        "PRETRAINED_BASE_MODEL = \"google/flan-t5-base\"  # @param {type:\"string\"}\n",
        "MAX_INPUT_LENGTH = 1024  # @param {type:\"integer\"}\n",
        "MAX_OUTPUT_LENGTH = 256  # @param {type:\"integer\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4880c1dc-c88a-43f7-a25c-a7fcb9c410ae",
      "metadata": {
        "id": "4880c1dc-c88a-43f7-a25c-a7fcb9c410ae"
      },
      "source": [
        "## a) 準備分詞器並對資料集進行分詞\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af7dceb4-582a-49b6-8fff-9ed709256863",
      "metadata": {
        "id": "af7dceb4-582a-49b6-8fff-9ed709256863"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_BASE_MODEL)\n",
        "\n",
        "\n",
        "def tokenize_function(examples: Dict[str, List[Any]]):\n",
        "    # Encode input to generate predictions and rationales\n",
        "    model_inputs = tokenizer(\n",
        "        [\"predict: \" + text for text in examples[\"input\"]],\n",
        "        max_length=MAX_INPUT_LENGTH,\n",
        "        truncation=True,\n",
        "    )\n",
        "    expl_model_inputs = tokenizer(\n",
        "        [\"explain: \" + text for text in examples[\"input\"]],\n",
        "        max_length=MAX_INPUT_LENGTH,\n",
        "        truncation=True,\n",
        "    )\n",
        "    model_inputs[\"expl_input_ids\"] = expl_model_inputs[\"input_ids\"]\n",
        "    model_inputs[\"expl_attention_mask\"] = expl_model_inputs[\"attention_mask\"]\n",
        "\n",
        "    # Encode target label and target rationale\n",
        "    label_output_encodings = tokenizer(\n",
        "        text_target=examples[\"label\"], max_length=MAX_OUTPUT_LENGTH, truncation=True\n",
        "    )\n",
        "    rationale_output_encodings = tokenizer(\n",
        "        text_target=examples[\"llm_rationale\"],\n",
        "        max_length=MAX_OUTPUT_LENGTH,\n",
        "        truncation=True,\n",
        "    )\n",
        "    model_inputs[\"labels\"] = label_output_encodings[\"input_ids\"]\n",
        "    model_inputs[\"expl_labels\"] = rationale_output_encodings[\"input_ids\"]\n",
        "\n",
        "    return model_inputs\n",
        "\n",
        "\n",
        "tokenized_dataset = dataset.map(\n",
        "    tokenize_function,\n",
        "    remove_columns=[\"input\", \"llm_rationale\", \"label\", \"llm_label\"],\n",
        "    batched=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4eb46c2-961b-4c30-a175-bdd36a325a88",
      "metadata": {
        "id": "c4eb46c2-961b-4c30-a175-bdd36a325a88"
      },
      "source": [
        "## b) 準備模型\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c3eab9b-ee7d-49da-8875-c14693358e4f",
      "metadata": {
        "id": "1c3eab9b-ee7d-49da-8875-c14693358e4f"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForSeq2SeqLM.from_pretrained(PRETRAINED_BASE_MODEL)\n",
        "# Uncomment if you have more than one GPU to enable parallelism\n",
        "# model.parallelize()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9507d2e-6e78-4b38-ac22-208b9b81a791",
      "metadata": {
        "id": "b9507d2e-6e78-4b38-ac22-208b9b81a791"
      },
      "source": [
        "## c) 為多任務訓練準備數據整理器\n",
        "由於我們需要為答案和原理生成預測，對每個訓練和預測步驟，我們將使用自訂的 DataCollator，它會取每批功能並回傳兩組功能和標籤，每組分別是答案和原理\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6695644-b788-4bab-ba72-cb8a2789ad3f",
      "metadata": {
        "id": "e6695644-b788-4bab-ba72-cb8a2789ad3f"
      },
      "outputs": [],
      "source": [
        "class TaskPrefixDataCollator(DataCollatorForSeq2Seq):\n",
        "    def __call__(self, features, return_tensors=None):\n",
        "        features_df = pd.DataFrame(features)\n",
        "\n",
        "        # Generate features for answers\n",
        "        ans_features = features_df.loc[\n",
        "            :, features_df.columns.isin([\"labels\", \"input_ids\", \"attention_mask\"])\n",
        "        ].to_dict(\"records\")\n",
        "        ans_features = super().__call__(ans_features, return_tensors)\n",
        "\n",
        "        # Generate features for explanations\n",
        "        expl_features = (\n",
        "            features_df.loc[\n",
        "                :,\n",
        "                features_df.columns.isin(\n",
        "                    [\"expl_labels\", \"expl_input_ids\", \"expl_attention_mask\"]\n",
        "                ),\n",
        "            ]\n",
        "            .rename(\n",
        "                columns={\n",
        "                    \"expl_labels\": \"labels\",\n",
        "                    \"expl_input_ids\": \"input_ids\",\n",
        "                    \"expl_attention_mask\": \"attention_mask\",\n",
        "                }\n",
        "            )\n",
        "            .to_dict(\"records\")\n",
        "        )\n",
        "        expl_features = super().__call__(expl_features, return_tensors)\n",
        "\n",
        "        return {\n",
        "            \"ans\": ans_features,\n",
        "            \"expl\": expl_features,\n",
        "        }\n",
        "\n",
        "\n",
        "data_collator = TaskPrefixDataCollator(tokenizer=tokenizer, model=model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "552e5c8c-0830-43ad-bcc8-ccdbfc8c3776",
      "metadata": {
        "id": "552e5c8c-0830-43ad-bcc8-ccdbfc8c3776"
      },
      "source": [
        "## d) 準備訓練器進行多任務訓練\n",
        "與此類似，我們將使用自訂訓練器來訓練模型，模型考量了答案生成和依據生成中兩種損失。我們會使用超參數 `alpha` 來控制這兩個損失對於總體模型損失的相對貢獻\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58f0083b-b494-4dd8-ab58-29f850d2fa89",
      "metadata": {
        "id": "58f0083b-b494-4dd8-ab58-29f850d2fa89"
      },
      "outputs": [],
      "source": [
        "class TaskPrefixTrainer(Seq2SeqTrainer):\n",
        "    def __init__(self, alpha, output_rationale, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.alpha = alpha\n",
        "        self.output_rationale = output_rationale\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        ans_outputs = model(**inputs[\"ans\"])\n",
        "        expl_outputs = model(**inputs[\"expl\"])\n",
        "\n",
        "        loss = self.alpha * ans_outputs.loss + (1.0 - self.alpha) * expl_outputs.loss\n",
        "\n",
        "        return (\n",
        "            (loss, {\"ans\": ans_outputs, \"expl\": expl_outputs})\n",
        "            if return_outputs\n",
        "            else loss\n",
        "        )\n",
        "\n",
        "    def prediction_step(self, model, inputs, prediction_loss_only, ignore_keys=None):\n",
        "        ans_outputs = super().prediction_step(\n",
        "            model, inputs[\"ans\"], prediction_loss_only=False, ignore_keys=ignore_keys\n",
        "        )\n",
        "        if self.output_rationale:\n",
        "            expl_outputs = super().prediction_step(\n",
        "                model,\n",
        "                inputs[\"expl\"],\n",
        "                prediction_loss_only=False,\n",
        "                ignore_keys=ignore_keys,\n",
        "            )\n",
        "        else:\n",
        "            expl_outputs = ans_outputs  # placeholder only\n",
        "\n",
        "        loss = self.alpha * ans_outputs[0] + (1 - self.alpha) * expl_outputs[0]\n",
        "\n",
        "        return (\n",
        "            loss,\n",
        "            [ans_outputs[1], expl_outputs[1]],\n",
        "            [ans_outputs[2], expl_outputs[2]],\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9019019-957d-4303-9c29-e07908ff6bbe",
      "metadata": {
        "id": "a9019019-957d-4303-9c29-e07908ff6bbe"
      },
      "source": [
        "# 步驟 3：訓練模型\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82d06883-60c5-4916-b9d7-6ff82e2b40e3",
      "metadata": {
        "id": "82d06883-60c5-4916-b9d7-6ff82e2b40e3"
      },
      "outputs": [],
      "source": [
        "from transformers import Seq2SeqTrainingArguments\n",
        "from transformers.trainer_utils import set_seed\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc86b1b2-1ff9-4249-b259-8d80a2c2c086",
      "metadata": {
        "id": "bc86b1b2-1ff9-4249-b259-8d80a2c2c086"
      },
      "outputs": [],
      "source": [
        "RUN_ID = 0  # @param {type:\"integer\"}\n",
        "CONFIG_DIR = \"distillation_outputs\"  # @param {type:\"string\"}\n",
        "CKPT_DIR = f\"{CONFIG_DIR}/ckpts/{RUN_ID}\"  # for model checkpoints\n",
        "LOG_DIR = f\"{CONFIG_DIR}/logs/{RUN_ID}\"  # for training logs\n",
        "\n",
        "EVAL_STEPS = 500  # @param {type:\"integer\"}\n",
        "SAVE_STEPS = 1000  # @param {type:\"integer\"}\n",
        "MAX_STEPS = 4000  # @param {type:\"integer\"}\n",
        "\n",
        "LEARNING_RATE = 5e-5\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "ALPHA = 0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06445292-4027-444b-8a59-cb2ace9e808a",
      "metadata": {
        "id": "06445292-4027-444b-8a59-cb2ace9e808a"
      },
      "outputs": [],
      "source": [
        "set_seed(RUN_ID)\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    CKPT_DIR,\n",
        "    remove_unused_columns=False,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=EVAL_STEPS,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=SAVE_STEPS,\n",
        "    logging_dir=LOG_DIR,\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=EVAL_STEPS,\n",
        "    max_steps=MAX_STEPS,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    gradient_accumulation_steps=1,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    predict_with_generate=True,\n",
        "    seed=RUN_ID,\n",
        "    local_rank=-1,\n",
        "    bf16=False,\n",
        "    generation_max_length=64,\n",
        "    prediction_loss_only=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e470d3a-8598-4899-8a45-b5ee92704198",
      "metadata": {
        "id": "2e470d3a-8598-4899-8a45-b5ee92704198"
      },
      "outputs": [],
      "source": [
        "from typing import Tuple, Callable\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "\n",
        "def compute_metrics_text(tokenizer: AutoTokenizer) -> Callable:\n",
        "    def compute_metrics(eval_pred: Tuple[np.ndarray, np.ndarray]) -> Dict[str, float]:\n",
        "        predictions, labels = eval_pred\n",
        "        decoded_preds = tokenizer.batch_decode(predictions[0], skip_special_tokens=True)\n",
        "\n",
        "        labels = np.where(labels[0] != -100, labels[0], tokenizer.pad_token_id)\n",
        "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "        acc = np.mean(np.array(decoded_preds) == np.array(decoded_labels))\n",
        "\n",
        "        return {\"accuracy\": acc}\n",
        "\n",
        "    return compute_metrics\n",
        "\n",
        "\n",
        "compute_metrics = compute_metrics_text(tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8ccb217-b010-4481-8607-e98363bb3776",
      "metadata": {
        "id": "b8ccb217-b010-4481-8607-e98363bb3776"
      },
      "outputs": [],
      "source": [
        "trainer_kwargs = {\n",
        "    \"alpha\": ALPHA,\n",
        "    \"output_rationale\": False,\n",
        "    \"model\": model,\n",
        "    \"args\": training_args,\n",
        "    \"train_dataset\": tokenized_dataset[\"train\"],\n",
        "    \"eval_dataset\": {\n",
        "        \"test\": tokenized_dataset[\"test\"],\n",
        "    },\n",
        "    \"data_collator\": data_collator,\n",
        "    \"tokenizer\": tokenizer,\n",
        "    \"compute_metrics\": compute_metrics,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b687fe1b-40ad-4ae0-bdf2-cd4a50c2154c",
      "metadata": {
        "id": "b687fe1b-40ad-4ae0-bdf2-cd4a50c2154c"
      },
      "outputs": [],
      "source": [
        "trainer = TaskPrefixTrainer(**trainer_kwargs)\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1cde36bb-55c8-4e8b-88f0-a63493c22503",
      "metadata": {
        "id": "1cde36bb-55c8-4e8b-88f0-a63493c22503"
      },
      "source": [
        "# 步驟 4：評估模型\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "394bb4ab-ce55-43f3-b8a2-ae01afd0f578",
      "metadata": {
        "id": "394bb4ab-ce55-43f3-b8a2-ae01afd0f578"
      },
      "source": [
        "現在讓我們將我們蒸餾的學生模型的效能與 PaLM 模型進行比較。我們也會嘗試從基礎學生模型中產生產出，以比較蒸餾訓練方法所造成的差異。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "569ac9c6-dc0b-4bbc-9403-b9447055761b",
      "metadata": {
        "id": "569ac9c6-dc0b-4bbc-9403-b9447055761b"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b54f85d-450a-474a-b005-15f44f594a3d",
      "metadata": {
        "id": "5b54f85d-450a-474a-b005-15f44f594a3d"
      },
      "outputs": [],
      "source": [
        "CHECKPOINT = f\"{CKPT_DIR}/checkpoint-4000\"\n",
        "\n",
        "distilled_tokenizer = AutoTokenizer.from_pretrained(CHECKPOINT)\n",
        "distilled_model = AutoModelForSeq2SeqLM.from_pretrained(CHECKPOINT)\n",
        "\n",
        "base_tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_BASE_MODEL)\n",
        "base_model = AutoModelForSeq2SeqLM.from_pretrained(PRETRAINED_BASE_MODEL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9eafc3e5-6116-4370-be03-96a88c08fc0f",
      "metadata": {
        "id": "9eafc3e5-6116-4370-be03-96a88c08fc0f"
      },
      "outputs": [],
      "source": [
        "distill_generator = pipeline(\n",
        "    \"text2text-generation\", model=distilled_model, tokenizer=distilled_tokenizer\n",
        ")\n",
        "base_generator = pipeline(\n",
        "    \"text2text-generation\", model=base_model, tokenizer=base_tokenizer\n",
        ")\n",
        "\n",
        "\n",
        "def generate_answers(sample: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    sample[\"distill_label\"] = distill_generator([\"predict: \" + sample[\"input\"]])[0][\n",
        "        \"generated_text\"\n",
        "    ]\n",
        "    sample[\"base_label\"] = base_generator(sample[\"input\"])[0][\"generated_text\"]\n",
        "    return sample\n",
        "\n",
        "\n",
        "output_dataset = dataset[\"test\"].map(generate_answers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afc3a499-0861-42b8-9d30-953d08b32543",
      "metadata": {
        "id": "afc3a499-0861-42b8-9d30-953d08b32543"
      },
      "outputs": [],
      "source": [
        "output_df = output_dataset.to_pandas().drop(\"llm_rationale\", axis=1)\n",
        "display_df = output_df.copy().rename(\n",
        "    columns={\n",
        "        \"input\": \"Question\",\n",
        "        \"label\": \"True answer\",\n",
        "        \"llm_label\": \"PaLM answer\",\n",
        "        \"base_label\": \"T5 answer\",\n",
        "        \"distill_label\": \"Distilled T5 answer\",\n",
        "    }\n",
        ")\n",
        "display_df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e05aed0-57ab-4097-aad0-74bef2d7451f",
      "metadata": {
        "id": "8e05aed0-57ab-4097-aad0-74bef2d7451f"
      },
      "outputs": [],
      "source": [
        "print(\n",
        "    \"The accuracy of PaLM model is {:.2f}%\".format(\n",
        "        output_df[output_df[\"label\"] == output_df[\"llm_label\"]][\"label\"].count()\n",
        "        / len(output_df)\n",
        "        * 100\n",
        "    )\n",
        ")\n",
        "print(\n",
        "    \"The accuracy of raw student model is {:.2f}%\".format(\n",
        "        output_df[output_df[\"label\"] == output_df[\"base_label\"]][\"label\"].count()\n",
        "        / len(output_df)\n",
        "        * 100\n",
        "    )\n",
        ")\n",
        "print(\n",
        "    \"The accuracy of distilled student model is {:.2f}%\".format(\n",
        "        output_df[output_df[\"label\"] == output_df[\"distill_label\"]][\"label\"].count()\n",
        "        / len(output_df)\n",
        "        * 100\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec44724e-a62c-4ce1-af69-5bdf268e6aaf",
      "metadata": {
        "id": "ec44724e-a62c-4ce1-af69-5bdf268e6aaf"
      },
      "source": [
        "正如我們所見，原始預訓練學生模型無法產生答案。然而，只需幾個訓練樣本和次世代，我們便已能夠使用小得多的 T5 模型接近 PaLM 模型的準確率。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9498af79-57dd-47e0-9072-5507b5527da7",
      "metadata": {
        "id": "9498af79-57dd-47e0-9072-5507b5527da7"
      },
      "source": [
        "# 第 5 步：將模型部署至 Vertex AI\n",
        "*備註：下列步驟將建立一個具有指定名稱的 Cloud Storage 儲存空間和一個 Artifact Registry Docker 儲存庫。如果你想使用現有的儲存空間或儲存庫，請在下方提供其名稱，並註解出建立資源的步驟，如下所示*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a31f785-399a-46c3-9fac-0d1dbb47706d",
      "metadata": {
        "id": "8a31f785-399a-46c3-9fac-0d1dbb47706d"
      },
      "outputs": [],
      "source": [
        "STAGING_BUCKET = \"\"  # @param {type:\"string\"}\n",
        "ARTIFACTS_DIR = f\"{STAGING_BUCKET}/distilled-t5\"\n",
        "CHECKPOINT_STEP = 4000  # @param {type:\"integer\"}\n",
        "CHECKPOINT = f\"{CKPT_DIR}/checkpoint-{CHECKPOINT_STEP}\"\n",
        "DOCKER_REPO_NAME = \"distill-step-by-step\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e37e1e8f-f224-47b2-aaed-1ce2fa69e254",
      "metadata": {
        "id": "e37e1e8f-f224-47b2-aaed-1ce2fa69e254"
      },
      "source": [
        "## 將產品發送到雲端儲存空間\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5656f337-5cb0-4b87-a350-ab0412ae8b91",
      "metadata": {
        "id": "5656f337-5cb0-4b87-a350-ab0412ae8b91"
      },
      "outputs": [],
      "source": [
        "! gsutil mb gs://{STAGING_BUCKET} # comment to use existing bucket\n",
        "! gsutil -m cp {CHECKPOINT}/* gs://{ARTIFACTS_DIR}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09255866-e1a4-4dba-aa87-bb256f11caba",
      "metadata": {
        "id": "09255866-e1a4-4dba-aa87-bb256f11caba"
      },
      "source": [
        "## 建立模型服務容器\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9db4f85a-d178-42cb-b4c0-190892014960",
      "metadata": {
        "id": "9db4f85a-d178-42cb-b4c0-190892014960"
      },
      "outputs": [],
      "source": [
        "!gcloud artifacts repositories create {DOCKER_REPO_NAME} --location {REGION} --repository-format=docker  # comment to use existing bucket\n",
        "!gcloud auth configure-docker {REGION}-docker.pkg.dev --quiet\n",
        "!gcloud builds submit --tag {REGION}-docker.pkg.dev/{PROJECT}/{DOCKER_REPO_NAME}/distilled-flan-t5:latest ./prediction_container"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d3fb839-068b-49fd-9b73-85010487358f",
      "metadata": {
        "id": "5d3fb839-068b-49fd-9b73-85010487358f"
      },
      "source": [
        "## 上傳模型\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76afb3a9-b982-49ef-885c-8ced4cfd3eb8",
      "metadata": {
        "id": "76afb3a9-b982-49ef-885c-8ced4cfd3eb8"
      },
      "outputs": [],
      "source": [
        "from google.cloud import aiplatform\n",
        "\n",
        "aiplatform.init(project=PROJECT, location=REGION, staging_bucket=STAGING_BUCKET)\n",
        "\n",
        "DEPLOY_IMAGE = (\n",
        "    f\"{REGION}-docker.pkg.dev/{PROJECT}/{DOCKER_REPO_NAME}/distilled-flan-t5:latest\"\n",
        ")\n",
        "HEALTH_ROUTE = \"/health\"\n",
        "PREDICT_ROUTE = \"/predict\"\n",
        "SERVING_CONTAINER_PORTS = [7080]\n",
        "\n",
        "model = aiplatform.Model.upload(\n",
        "    display_name=f\"distilled-flan-t5\",\n",
        "    description=f\"Distilled Flan T5 model using Step-By-Step Distillation\",\n",
        "    serving_container_image_uri=DEPLOY_IMAGE,\n",
        "    serving_container_predict_route=PREDICT_ROUTE,\n",
        "    serving_container_health_route=HEALTH_ROUTE,\n",
        "    serving_container_ports=SERVING_CONTAINER_PORTS,\n",
        "    artifact_uri=f\"gs://{ARTIFACTS_DIR}\",\n",
        ")\n",
        "print(model.resource_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16abe463-0673-4741-b73f-c7b02167d6ed",
      "metadata": {
        "id": "16abe463-0673-4741-b73f-c7b02167d6ed"
      },
      "source": [
        "## 部署模型\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2e9e798-f35c-4b28-8e47-5e8ee84c68ec",
      "metadata": {
        "id": "e2e9e798-f35c-4b28-8e47-5e8ee84c68ec"
      },
      "outputs": [],
      "source": [
        "model = aiplatform.Model(model.resource_name)\n",
        "\n",
        "endpoint = model.deploy(\n",
        "    machine_type=\"n1-standard-4\",\n",
        "    traffic_split={\"0\": 100},\n",
        "    min_replica_count=1,\n",
        "    max_replica_count=1,\n",
        "    accelerator_type=\"NVIDIA_TESLA_T4\",\n",
        "    accelerator_count=1,\n",
        "    traffic_percentage=100,\n",
        "    deploy_request_timeout=1200,\n",
        "    sync=True,\n",
        ")\n",
        "endpoint.wait()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "AuBi541Ijkq2",
      "metadata": {
        "id": "AuBi541Ijkq2"
      },
      "source": [
        "# 總結\n",
        "在這個筆記本中，我們學到如何透過讓大型老師 LLM 來教導較小的學生 LLM 思考，這將明顯改善較小型模型在簡單指令微調上的效能。\n",
        "\n",
        "如果你有興趣在 Google Cloud 上運作 LLM 上類似的擴散程序，請查看 [在 Google Cloud 上擴散文字模型](https://cloud.google.com/vertex-ai/docs/generative-ai/models/distill-text-models/)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "distillation (Local)",
      "language": "python",
      "name": "local-distillation"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}