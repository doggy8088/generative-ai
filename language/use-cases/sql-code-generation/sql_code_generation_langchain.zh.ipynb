{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47bgjiSzeJnA"
      },
      "outputs": [],
      "source": [
        "# Copyright 2023 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DF9B5dFQeLu7"
      },
      "source": [
        "# 使用 LangChain 在 Vertex AI 上進行 SQL 程式碼產生 🦜🔗\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/doggy8088/generative-ai/blob/main/language/use-cases/sql-code-generation/sql_code_generation_langchain.zh.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Google Colaboratory logo\"><br> 在 Colab 中執行\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/doggy8088/generative-ai/blob/main/language/use-cases/sql-code-generation/sql_code_generation_langchain.zh.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\"><br> 在 GitHub 上檢視\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/doggy8088/generative-ai/blob/main/language/use-cases/sql-code-generation/sql_code_generation_langchain.zh.ipynb\">\n",
        "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br> 在 Vertex AI Workbench 中開啟\n",
        "    </a>\n",
        "  </td>\n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "| | |\n",
        "|-|-|\n",
        "| 作者: | [Shubham Chawla](https://www.github.com/shubhamgoogle), [Roy Arsan](https://www.linkedin.com/in/arsan) |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvryK2kGeWfP"
      },
      "source": [
        "## 概述\n",
        "大語言模型可於產生程式碼，包含 SQL。特別是，模型可以將自然語言文字轉換成 SQL 查詢。其中一個常見的目的是讓使用者在不需要知道表格名稱、資料架構及基礎資料倉儲中的特定 SQL 方言或查詢引擎 (例如 BigQuery) 的情況下，也可以查詢資料。\n",
        "\n",
        "本記事涵蓋 SQL 程式碼產生中提示工程的最佳實務，以及透過 [Langchain Google Cloud Vertex AI](https://python.langchain.com/docs/integrations/llms/google_vertex_ai_palm) 執行，並於 [SQL-PaLM: 改善大型語言模型適應，以用於文字轉 SQL](https://arxiv.org/pdf/2306.00739.pdf) 中實行所學到的知識。例如，BigQuery 資料集架構會以動態方式擷取，並作為提示的內容提供，以奠定 LLM 的基礎並個人化其輸出。本記事也示範檢索加強產生 (RAG)，方法是使用來自 Langchain 範例選擇器的 [SemanticSimilarityExampleSelector](https://python.langchain.com/docs/modules/model_io/prompts/example_selector_types/similarity) 來動態擷取並傳遞最相關的幾種範例到豐富的 LLM 提示。這有助於確保最準確、最相關的 LLM 輸出，也就是產生的 SQL 查詢，同時限制所需的 LLM 輸入 Token 數量，進而降低成本。本記事也示範簡單的模型評估，其中產生的 SQL 查詢透過針對 BigQuery 資料集執行來評估，並透過將其與真實查詢及相應結果進行比較。\n",
        "\n",
        "在本記事中，你會產生 SQL 查詢來分析 Cloud Audit Logs，並回答你自己的 Google Cloud 專案中的活動周遭關鍵安全性問題。雖然本文使用 BigQuery 日誌資料集，但本文提出的概念和方法也可以套用於其他資料庫與資料集。\n",
        "\n",
        "![NL2SQL flow](https://services.google.com/fh/files/misc/nl2sql_for_log_analytics2.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ool643Tzg7W2"
      },
      "source": [
        "### 目標\n",
        "\n",
        "在完成筆記本後，你將可以：\n",
        "\n",
        "* 使用模型根據自然語言問題產生 SQL 查詢：\n",
        "  * 使用 VertexAIEmbeddings 建立內嵌\n",
        "  * 使用 Langchain 範例選擇器自動為小量示範提示選擇相關範例\n",
        "  * 提供自訂資料集架構作為內容\n",
        "  * 格式化模型輸出\n",
        "\n",
        "* 評估模型產生之查詢，方法為：\n",
        "  * 對實際資料集執行已調整的查詢\n",
        "  * 使用 pandas 資料框相等檢查，將查詢 (及其結果) 與 ground truth 查詢進行比較\n",
        "  * 計算模型準確度分數\n",
        "\n",
        "此外，你可以使用這個筆記本從你自己的稽核記錄中回答你自己的安全性問題，例如：\n",
        "\n",
        "- 過去一個月內，是否有不正常的高雲端 API 使用量由任何使用者身分使用？\n",
        "- 過去 7 天內，是否有不適當的身分執行任何破壞性行為？\n",
        "- 本週內，是否有使用者存取到異常大量資料，導致每日資料量暴增？\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqxB1023g_fM"
      },
      "source": [
        "## 開始使用\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2Vfi5W0hNto"
      },
      "source": [
        "### 必備條件\n",
        "如果你尚未執行此操作，唯一的需求是將你現有的記錄區塊 [升級](https://cloud.google.com/logging/docs/buckets#upgrade-bucket) 至 Log Analytics，這樣便能為你連結一個 BigQuery 資料集，裡面有可進行查詢的記錄資料。這是 **一鍵完成步驟，不會產生額外的費用** 。預設情況下，每個專案的 _Required_ 區塊會啟用、擷取並儲存 Cloud Audit 管理員活動記錄，不會產生任何費用。\n",
        "\n",
        "![one click prerequisite](https://services.google.com/fh/files/misc/upgrade_log_bucket.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIZYzBgHhT6R"
      },
      "source": [
        "### 安裝 SDK\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z3EARCs46U-I"
      },
      "outputs": [],
      "source": [
        "# Install Vertex AI SDK to use for model predictions\n",
        "!pip install google-cloud-aiplatform google-cloud-bigquery --upgrade --user\n",
        "!pip install langchain\n",
        "!pip install --upgrade --quiet  langchain-core langchain-google-vertexai\n",
        "!pip install --upgrade --quiet langchain langchain-google-vertexai\n",
        "!pip install faiss-cpu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbScDyV5hf1D"
      },
      "source": [
        "**僅限 Colab：** 取消以下Cell註解以重新啟動核心或使用按鈕重新啟動核心。對於 Vertex AI Workbench，你可以使用右上方的按鈕重新啟動終端機。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8eJdvWP-Dq35"
      },
      "outputs": [],
      "source": [
        "# # Automatically restart kernel after installs so that your environment can access the new packages\n",
        "# import IPython\n",
        "# app = IPython.Application.instance()\n",
        "# app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akTXroE0ChYP"
      },
      "source": [
        "### 引進函式庫\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gBMjDTw59cFt"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
        "from langchain.prompts.prompt import PromptTemplate\n",
        "from langchain_google_vertexai import VertexAIEmbeddings, VertexAI\n",
        "from langchain.prompts.example_selector import SemanticSimilarityExampleSelector\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.cloud import bigquery\n",
        "from google.cloud import aiplatform\n",
        "import sys"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akW4u3cIjLG7"
      },
      "source": [
        "### 為 BigQuery 設定專案和資料集\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZWnTdzIjLM6"
      },
      "source": [
        "這是包含下列項目的專案：\n",
        "- 連結的 BigQuery 資料集 `BQ_LINKED_DATASET` (含你的原始記錄)，以及，\n",
        "- 新的 BigQuery 資料集 `BQ_PROCESSED_DATASET`，用來儲存已處理的記錄。\n",
        "\n",
        "這個專案可以是你用於 Vertex AI 的專案，也可以是不同的專案。\n",
        "\n",
        "請確定你有 `BQ_LINKED_DATASET` 資料集的 **BigQuery Data Viewer** 角色\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-mYUuAS7CeX"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"\"  # @param {type:\"string\"}\n",
        "LOCATION_US = \"US\"  # @param {type:\"string\"}\n",
        "LOCATION = \"us-central1\"  # @param {type:\"string\n",
        "BQ_LINKED_DATASET = \"\"  # @param {type:\"string\"}\n",
        "BQ_PROCESSED_DATASET = \"\"  # @param {type:\"string\"}\n",
        "\n",
        "aiplatform.init(project=PROJECT_ID, location=LOCATION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66PtfF1PhnEV"
      },
      "source": [
        "### 驗證你的筆記本電腦環境\n",
        "* 如果你使用 **Colab** 執行此筆記本電腦，執行下列Cell並繼續。\n",
        "* 如果你使用 **Vertex AI 工作台** ，請查看 [這裡](https://github.com/doggy8088/generative-ai/tree/main/setup-env) 的設定說明。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WPy4O74t_LU2"
      },
      "outputs": [],
      "source": [
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhcMpyDBKexo"
      },
      "source": [
        "### 設定嵌入和 Vertex AI LLM 模型\n",
        "\n",
        "在目前的範例，我們正在使用 text-bison@002 大型語言模型，但你可以使用 Google 提供的其他模型 gemini、gemini-pro、ulta 等。對於嵌入，我們使用最新版本的 textembedding-gecko。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q4vG3p5oKd92"
      },
      "outputs": [],
      "source": [
        "MODEL_ID = \"text-bison@002\"  # @param {type:\"string\"}\n",
        "EMBEDDING_MODEL_ID = \"textembedding-gecko@latest\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sipwSsYKCEMe"
      },
      "source": [
        "### 匯入樣本查詢\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIbiuWUTCHDD"
      },
      "source": [
        "你現在會從CSV文件裡擷取15個範例安全問題和相應的SQL查詢清單。這些安全問題是變動自開原始碼[社群安全分析](https://github.com/GoogleCloudPlatform/security-analytics)的。CSA提供一組安全問題以及對應的查詢，適用於BigQuery、Log Analytics和Chronicle。\n",
        "\n",
        "我們會使用這些查詢的子集作為模型提示範例中的小樣本，以及將剩餘的部分用於模型評估。\n",
        "\n",
        "執行下列命令，從GCS儲存區讀取CSV文件，並載入所有記錄至記憶中的pandas DataFrame中：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O_9ZBVhjIXFI"
      },
      "outputs": [],
      "source": [
        "BUCKET_ID = \"csa-datasets-public\"  # @param {type:\"string\"}\n",
        "FILENAME = \"SQL_Generator_Example_Queries.csv\"  # @param {type:\"string\"}\n",
        "df = pd.read_csv(f\"gs://{BUCKET_ID}/{FILENAME}\", header=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QBZwTUnDnuC"
      },
      "source": [
        "### 萃取訓練 & 評估資料集\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPoHff6CDslh"
      },
      "source": [
        "提取訓練和評估資料集，並儲存在各自的資料架構中：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x24U6jDzDo0E"
      },
      "outputs": [],
      "source": [
        "train_df = df.loc[df[\"Dataset\"] == \"Train\", [\"Question\", \"SQL Query\"]]\n",
        "eval_df = df.loc[df[\"Dataset\"] == \"Eval\", [\"Question\", \"SQL Query\"]]\n",
        "train_dict = (\n",
        "    train_df[[\"Question\", \"SQL Query\"]]\n",
        "    .rename(columns={\"SQL Query\": \"answer\"})\n",
        "    .rename(columns={\"Question\": \"question\"})\n",
        "    .to_dict(orient=\"records\")\n",
        ")\n",
        "train_df.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6AtPphSBjxn"
      },
      "source": [
        "## 準備資料\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GC6Wbd6EBnM4"
      },
      "source": [
        "> 如果你已使用 [Dataform 作為 Community Security Analytics 的一部分](https://github.com/GoogleCloudPlatform/security-analytics/tree/main/dataform) (CSA) 來處理原始記錄並使用經過整理的表格進行標準化，可以略過此部分。如需瞭解 CSA 以及如何自動持續建立經過後製處理的資料表以取代原始記錄，請查看 [Google Cloud 部落格文章](https://cloud.google.com/blog/products/data-analytics/deploy-community-security-analytics-with-dataform)。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HB1Ki-d5Bp75"
      },
      "source": [
        "如同任何其他的人工智慧 /機器學習專案，第一件事便是準備好資料，包括給少次提示和後續評估所用的資料集。你會將儲存在 BigQuery 連結資料集的原始記錄預先處理成摘要表格，並放到你的新 BigQuery 資料集裡。這個表格將以彙總方式顯示記錄，並正規化成一個簡單的架構。這讓你可以解鎖和縮放機器學習分析：\n",
        "- 從運算觀點而言，因為這個資料集較小且簡單。\n",
        "- 從人才觀點而言，因為研究人員和分析師不一定要熟悉原始記錄的複雜架構 ( [Log Entry 定義](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry))。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gR65cweHLbA_"
      },
      "source": [
        "### 建立新資料集\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yJ4ufY5pLcEK"
      },
      "outputs": [],
      "source": [
        "!bq --location=LOCATION_US mk --dataset {BQ_PROJECT_ID}:{BQ_PROCESSED_DATASET}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pX3XEZ4MLMac"
      },
      "source": [
        "使用記錄分析 Bigquery 表格建立新的 csa_4_01_summary_daily\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "KWvGJw0C7YK2"
      },
      "outputs": [],
      "source": [
        "TABLE_NAME = \"csa_4_01_summary_daily\"\n",
        "TABLE_ID = f\"{PROJECT_ID}.{BQ_PROCESSED_DATASET}.{TABLE_NAME}\"\n",
        "SUMMARY_LOOKBACK_DAYS = 90\n",
        "\n",
        "client = bigquery.Client(project=PROJECT_ID, location=LOCATION_US)\n",
        "client.create_dataset(dataset=BQ_PROCESSED_DATASET, exists_ok=True)\n",
        "\n",
        "job_config = bigquery.QueryJobConfig(\n",
        "    destination=TABLE_ID, write_disposition=\"WRITE_TRUNCATE\"\n",
        ")\n",
        "\n",
        "sql = f\"\"\"\n",
        "SELECT\n",
        "  EXTRACT(DATE FROM timestamp) AS day,\n",
        "  proto_payload.audit_log.authentication_info.principal_email,\n",
        "  ARRAY_AGG(DISTINCT proto_payload.audit_log.method_name IGNORE NULLS) AS actions,\n",
        "  COUNT(*) AS counter\n",
        "FROM `{PROJECT_ID}.{BQ_LINKED_DATASET}._AllLogs`\n",
        "WHERE\n",
        "  timestamp >=  TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL {SUMMARY_LOOKBACK_DAYS} DAY)\n",
        "  AND proto_payload.audit_log.authentication_info.principal_email IS NOT NULL\n",
        "  AND proto_payload.audit_log.method_name NOT LIKE \"storage.%.get\"\n",
        "  AND proto_payload.audit_log.method_name NOT LIKE \"v1.compute.%.list\"\n",
        "  AND proto_payload.audit_log.method_name NOT LIKE \"beta.compute.%.list\"\n",
        "GROUP BY\n",
        "  day,\n",
        "  proto_payload.audit_log.authentication_info.principal_email\n",
        "\"\"\"\n",
        "\n",
        "# Start the query and save results in new table\n",
        "query_job = client.query(sql, job_config=job_config)\n",
        "result = query_job.result()  # Wait for the job to complete.\n",
        "\n",
        "print(f\"{result.total_rows} user action records loaded to table {TABLE_ID}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRA74CdhEODS"
      },
      "source": [
        "### 建構 schema 定義 (精簡版）\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJneRks9ERIt"
      },
      "source": [
        "首先，我們需要建立簡潔的資料集架構定義。如同前面所述，我們會將它用作提示背景的一部分來解決結果。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyZHLd-oEozF"
      },
      "source": [
        "從 BigQuery 資料集的 `INFORMATION_SCHEMA` 中擷取資料表和欄位定義。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "n49TC0PaJ23L"
      },
      "outputs": [],
      "source": [
        "# Following SQL query will generate schema definition of your dataset\n",
        "\n",
        "BQ_TABLES = df[\"Qualified table name\"].replace(\"\", np.nan).dropna().unique()\n",
        "print(BQ_TABLES)\n",
        "QUERY = f\"\"\"\\\n",
        "SELECT\n",
        "    '[Schema (values)]: ' || '| log_summary | ' || STRING_AGG(table_values, ' | ') || ';' AS tables_definition,\n",
        "    '[Column names (type)]: ' || STRING_AGG(column_names_types) || ';' AS columns_definition\n",
        "FROM (\n",
        "    SELECT\n",
        "      table_name,\n",
        "      table_name || ' : ' || STRING_AGG(column_name, ' , ') as table_values,\n",
        "      STRING_AGG(table_name || ' : ' || column_name || ' (' || data_type || ')', ' | ') as column_names_types\n",
        "    FROM {PROJECT_ID}.{BQ_PROCESSED_DATASET}.INFORMATION_SCHEMA.COLUMN_FIELD_PATHS\n",
        "    WHERE table_name IN {'(' + \",\".join(map(lambda x: f\"'{x}'\", BQ_TABLES)) + ')'}\n",
        "    GROUP BY table_name\n",
        "    ORDER BY table_name\n",
        ")\n",
        "\"\"\"\n",
        "\n",
        "# Create query job\n",
        "query_job = client.query(QUERY)\n",
        "# Get first row\n",
        "schema = next(query_job.result())\n",
        "\n",
        "# Build schema definition\n",
        "schema_definition = f\"\"\"\\\n",
        "{schema.tables_definition}\n",
        "\n",
        "{schema.columns_definition}\n",
        "\"\"\"\n",
        "\n",
        "print(schema_definition)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 建立提示\n",
        "\n",
        "我們使用使用者輸入和使用 Langchain FewShotPromptTemplate 動態提取的幾個鏡頭來建立提示\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 建立 VertexAI embeddings 以建立文字表徵\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "embedding = VertexAIEmbeddings(model_name=EMBEDDING_MODEL_ID, project=PROJECT_ID)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eh0vG61jR6Bb"
      },
      "source": [
        "#### 建立少量提示\n",
        "\n",
        "`SemanticSimilarityExampleSelector` 選擇範例的依據，為範例和輸入最相似的組合\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "IftnEU2SAiJ8"
      },
      "outputs": [],
      "source": [
        "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
        "    # This is the list of examples available to select from.\n",
        "    train_dict,\n",
        "    # This is the embedding class used to produce embeddings which are used to measure semantic similarity.\n",
        "    embedding,\n",
        "    # This is the VectorStore class that is used to store the embeddings and do a similarity search over.\n",
        "    FAISS,\n",
        "    # This is the number of examples to produce.\n",
        "    k=2,\n",
        ")\n",
        "\n",
        "# Select the most similar example to the input.\n",
        "question = \"select user actions that contains the word 'delete' or 'remove'\"\n",
        "selected_examples = example_selector.select_examples({\"question\": question})\n",
        "print(f\"Examples most similar to the input: {question}\")\n",
        "for example in selected_examples:\n",
        "    print(\"\\n\")\n",
        "    for k, v in example.items():\n",
        "        print(f\"{k}: {v}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 用於建立提示的 Helper 函式\n",
        "\n",
        "下方函式將用於將使用者輸入轉換成實際提示，其中包含少量範例和開頭\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_prompt(user_prompt, example_selector):\n",
        "    prompt_template = f\"\"\"\\\n",
        "    This is a task converting text into GoogleSQL statement.\n",
        "    We will first give you the dataset schema and then ask a question in text.\n",
        "    You are asked to generate SQL statement which is valid for BigQuery.\n",
        "    Remove any delimiters around answer such as \"```\"\n",
        "\n",
        "    BigQuery tables schema definition:\n",
        "    {schema_definition}\n",
        "    Here are a few shot examples:\n",
        "    \"\"\"\n",
        "    example_prompt = PromptTemplate(\n",
        "        input_variables=[\"question\", \"answer\"],\n",
        "        template=\"question: {question}\\nanswer: {answer}\",\n",
        "    )\n",
        "\n",
        "    prompt = FewShotPromptTemplate(\n",
        "        example_selector=example_selector,\n",
        "        example_prompt=example_prompt,\n",
        "        prefix=prompt_template,\n",
        "        suffix=\"question: {question}\\nanswer: \",\n",
        "        input_variables=[\"question\"],\n",
        "    )\n",
        "    final_prompt = prompt.format(question=user_prompt)\n",
        "    return final_prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhxjQ6F0Fol0"
      },
      "source": [
        "## 生成 SQL 查詢\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JK8aYpx0Fse0"
      },
      "source": [
        "### 定義用於產生 SQL 的輔助函式\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfqswuz2Fwqw"
      },
      "source": [
        "`generate_sql()`: 此函式用於從 Vertex AI LLM 模型中擷取 SQL 查詢，並使用我們迄今為止建置的提示範本。\n",
        "\n",
        "`execute_sql()`: 此函式用於針對實際 BigQuery 資料集執行 SQL 查詢，並將結果傳回為資料框。\n",
        "\n",
        "`build_prompt()`: 此函式用於建立最終提示，其中包含所有提示的共用字首和字尾\n",
        "\n",
        "請注意，`generate_sql()` 如何使用 `sanitize_output()` 函式在回傳結果前將回應分解為 SQL 查詢本身。即使模型提示包含調整模型輸出的指示，仍然可能會有需要移除的引號或程式區塊反引號，以避免後續的 SQL 語法錯誤。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O45Xw1uql0rF"
      },
      "outputs": [],
      "source": [
        "# Limit number of bytes processed as a guardrail for cost control\n",
        "BQ_MAX_BYTES_BILLED = pow(2, 30)  # 1GB\n",
        "\n",
        "\n",
        "def execute_sql(query: str):\n",
        "    print(\"Executing SQL...\")\n",
        "\n",
        "    # Qualify table names with your project and dataset ID\n",
        "    for table_name in BQ_TABLES:\n",
        "        query = query.replace(\n",
        "            table_name, f\"{PROJECT_ID}.{BQ_PROCESSED_DATASET}.{table_name}\"\n",
        "        )\n",
        "\n",
        "    # Validate the query by performing a dry run without incurring a charge\n",
        "    job_config = bigquery.QueryJobConfig(use_query_cache=False, dry_run=True)\n",
        "    try:\n",
        "        response = client.query(query, job_config=job_config)\n",
        "    except Exception as e:\n",
        "        print(\"Error validating query:\")\n",
        "        print(e)\n",
        "        return e\n",
        "\n",
        "    print(\"Query will process {:.2f} KB.\".format(response.total_bytes_processed / 1024))\n",
        "\n",
        "    # Execute the query\n",
        "    job_config = bigquery.QueryJobConfig(\n",
        "        use_query_cache=False, maximum_bytes_billed=BQ_MAX_BYTES_BILLED\n",
        "    )\n",
        "    try:\n",
        "        response = client.query(query)\n",
        "        df = response.to_dataframe()\n",
        "    except Exception as e:\n",
        "        print(\"Error executing query:\")\n",
        "        print(e)\n",
        "        return e\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "import re\n",
        "\n",
        "\n",
        "# Strip text to include only the SQL code block with\n",
        "def sanitize_output(text: str) -> str:\n",
        "    # Strip whitespace and any potential backticks enclosing the code block\n",
        "    text = text.strip()\n",
        "    regex = re.compile(r\"^\\s*```(\\w+)?|```\\s*$\")\n",
        "    text = regex.sub(\"\", text).strip()\n",
        "\n",
        "    # Find and remove any trailing quote without corresponding opening quote\n",
        "    if re.search(r'^[^\"]*\"$', text):\n",
        "        text = text[:-1]\n",
        "    # Find and remove any leading quote without corresponding closing quote\n",
        "    if re.search(r'^\"[^\"]*$', text):\n",
        "        text = text[1:]\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "# Call model using prompt and pre-defined parameters\n",
        "def generate_sql(\n",
        "    prompt: str,\n",
        "    temperature: float = 0.2,\n",
        "    max_output_tokens: int = 1024,\n",
        "    top_k: int = 40,\n",
        "    top_p: float = 0.8,\n",
        ") -> str:\n",
        "    print(\"Generating SQL...\")\n",
        "    print(\"Number of input tokens: \" + str(len(prompt)))\n",
        "\n",
        "    model = VertexAI(\n",
        "        model_name=MODEL_ID,\n",
        "        temperatore=temperature,\n",
        "        max_output_tokens=max_output_tokens,\n",
        "        top_k=top_k,\n",
        "        top_p=top_p,\n",
        "    )\n",
        "    final_prompt = build_prompt(prompt, example_selector)\n",
        "    print(final_prompt)\n",
        "    text = model.invoke(final_prompt)\n",
        "    print(\"Number of output tokens: \" + str(len(text)))\n",
        "    print(\"Response:\")\n",
        "    print(text)\n",
        "    # Strip text to include only the SQL code block\n",
        "    text = sanitize_output(text)\n",
        "    print(\"Response stripped:\")\n",
        "    print(text)\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--NVnQyodhAC"
      },
      "source": [
        "### 範例 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sR9hiB2Tdx53"
      },
      "source": [
        "讓我們生成 SQL 來回答這個範例問題：\n",
        "\n",
        "*列出包含「刪除」或「移除」字詞的所有使用者操作，時段涵蓋上個月。在結果中包含使用者與日期。*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Wmk0c2H_lhcB"
      },
      "outputs": [],
      "source": [
        "user_prompt = \"List all user actions that contains the word 'delete' or 'remove' over the last month. Include the user and the day in the results.\"\n",
        "\n",
        "final_generated_prompt = build_prompt(user_prompt, example_selector)\n",
        "print(final_generated_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kF0_OL5Odvdn"
      },
      "source": [
        "我們使用 BigQuery 中的線上資料集測試所產生的查詢。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "gEaH9nZbOloi"
      },
      "outputs": [],
      "source": [
        "output = generate_sql(example_selector, user_prompt)\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uzuKxoud6Cf"
      },
      "source": [
        "讓我們針對你的 BigQuery 資料集測試產生的查詢：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "HeNR3q_xPfO2"
      },
      "outputs": [],
      "source": [
        "# Execute the query\n",
        "query_result = execute_sql(output)\n",
        "display(query_result.head(2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vnb2LuGjLjpN"
      },
      "source": [
        "### 範例 2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hW-fLol-LjpX"
      },
      "source": [
        "讓我們生成 SQL 來回答這個範例問題：\n",
        "\n",
        "*列出包含「刪除」或「移除」字詞的所有使用者操作，時段涵蓋上個月。在結果中包含使用者與日期。*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "JyE38cipLjpY"
      },
      "outputs": [],
      "source": [
        "user_prompt = \"List any action containing IAM case-insensitive by any unapproved user over the last 7 days, where approved user include 'admin@example.com'\"\n",
        "\n",
        "final_generated_prompt = build_prompt(user_prompt, example_selector)\n",
        "print(final_generated_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAG4I30NLjpY"
      },
      "source": [
        "我們使用 BigQuery 中的線上資料集測試所產生的查詢。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "h_LksGE3LjpY"
      },
      "outputs": [],
      "source": [
        "output = generate_sql(example_selector, user_prompt)\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nqeQsWGLjpY"
      },
      "source": [
        "讓我們針對你的 BigQuery 資料集測試產生的查詢：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "PJq19QC_LjpY"
      },
      "outputs": [],
      "source": [
        "# Execute the query\n",
        "query_result = execute_sql(output)\n",
        "display(query_result.head(2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmI4izZRd-Id"
      },
      "source": [
        "## 評估模型\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3dkE8mCeFri"
      },
      "source": [
        "### 在評估資料集上運行模型\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmrMER40eI4O"
      },
      "source": [
        "讓我們為評估資料集中所有的問題產生 SQL 查詢。該資料集同時包括「問題」和正解「SQL 查詢」。執行以下程式碼，為資料集中的每個問題自動呼叫模型，並將回應記錄在新的欄位「已產生的 SQL 查詢」。由於模型呼叫是串列執行的，這可能需要幾分鐘。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "7UwUYtOEg-mf"
      },
      "outputs": [],
      "source": [
        "eval_df[\"Generated SQL Query\"] = eval_df[\"Question\"].apply(\n",
        "    lambda x: generate_sql(example_selector, x)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udQqttGIGNmM"
      },
      "source": [
        "### 比較輸出結果\n",
        "\n",
        "在下一單元格中，我們將執行原始的 SQL 查詢，然後直接比對它的輸出與所產生 SQL 查詢的輸出。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rffyi5XX1Giu"
      },
      "outputs": [],
      "source": [
        "def compare_dataframes(sql_query, generated_sql_query):\n",
        "    \"\"\"Compares two pandas DataFrames row-wise using columns from the second DataFrame.\n",
        "    Args:\n",
        "        SQL Query, Generated SQL Query\n",
        "    Returns:\n",
        "        True if output of both the SQL queries matches otherwise False\n",
        "    \"\"\"\n",
        "    df1 = execute_sql(sql_query)\n",
        "    df2 = execute_sql(generated_sql_query)\n",
        "\n",
        "    # If generated query returned an error instead of a dataframe with results:\n",
        "    if not isinstance(df2, pd.DataFrame):\n",
        "        return False\n",
        "\n",
        "    try:\n",
        "        df2 = df2[df1.columns]\n",
        "    except KeyError:\n",
        "        # Columns in results of ground truth query are missing\n",
        "        # from results returned by generated query\n",
        "        return False\n",
        "\n",
        "    comparison_result = df2.eq(df1)\n",
        "    matching_rows = comparison_result.all(axis=1)\n",
        "    matching_count = matching_rows.sum()\n",
        "    # return df1, df2\n",
        "    return True if matching_count == len(df1) else False\n",
        "\n",
        "\n",
        "eval_df[\"Data Match\"] = eval_df.apply(\n",
        "    lambda x: compare_dataframes(x[\"SQL Query\"], x[\"Generated SQL Query\"]), axis=1\n",
        ")\n",
        "# eval_df[\"sql_query_output\"],eval_df[\"generated_sql_query_output\"] = eval_df.apply(lambda x: compare_dataframes(x[\"SQL Query\"], x[\"Generated SQL Query\"]), axis=1)\n",
        "\n",
        "# Note: To save the output data to the final dataframe, make these changes: 1. Uncomment lines 26 and 30. 2. Comment out line 29."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydZeqe3FGUQf"
      },
      "source": [
        "## 最終結果\n",
        "\n",
        "在以下單元格中，我們將計算模型的最終分數。這個分數表示原始查詢和生成查詢之間成功匹配的百分比，如「資料配對」欄中所示。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-a85rHhNCs2"
      },
      "source": [
        "### 分數\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3SnMTlFSHV17"
      },
      "outputs": [],
      "source": [
        "def get_prcntg_match(eval_df):\n",
        "    return round(eval_df[\"Data Match\"].sum() / len(eval_df) * 100)\n",
        "\n",
        "\n",
        "prcntg_match = get_prcntg_match(eval_df)\n",
        "print(f\"Final Score based on the percentage of data match: {prcntg_match}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nQc4ePmIa-7"
      },
      "source": [
        "### 輸出\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "1eQxydrDEA-R"
      },
      "outputs": [],
      "source": [
        "pd.set_option(\"display.max_rows\", None)\n",
        "pd.set_option(\"display.max_columns\", None)\n",
        "pd.set_option(\"display.width\", None)\n",
        "pd.set_option(\"display.max_colwidth\", -1)\n",
        "display(eval_df)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}