{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uxCkB_DXTHzf"
      },
      "outputs": [],
      "source": [
        "# Copyright 2023 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hny4I-ODTIS6"
      },
      "source": [
        "# 使用 LangChain 總結大型文件 🦜🔗\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/doggy8088/generative-ai/blob/main/language/use-cases/document-summarization/summarization_large_documents_langchain.zh.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Google Colaboratory 圖示\"><br>在 Colab 上執行\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/doggy8088/generative-ai/blob/main/language/use-cases/document-summarization/summarization_large_documents_langchain.zh.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub 圖示\"><br>在 GitHub 上檢視\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/doggy8088/generative-ai/blob/main/language/use-cases/document-summarization/summarization_large_documents_langchain.zh.ipynb\">\n",
        "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI 圖示\"><br>在 Vertex AI Workbench 中開啟\n",
        "    </a>\n",
        "  </td>\n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "| | |\n",
        "|-|-|\n",
        "|作者 | [Polong Lin](https://github.com/polong-lin) |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nLS57E2TO5y"
      },
      "source": [
        "## 概述\n",
        "\n",
        "文本摘要化是一項 NLP 任務，用以建立較長篇幅文本的簡明且富含資訊的摘要。LLM 可用於建立新聞文章、研究論文、技術文件和其他類型文本的摘要。\n",
        "\n",
        "摘要化大型文件可能極具挑戰性。若要建立摘要，你需要針對索引文件套用摘要化策略。你已在之前的記事本中看過這些策略中的一些。如果你尚未完成，建議完成該作業以基本了解如何摘要化大型文件。\n",
        "\n",
        "在本記事本中，你將使用 LangChain (開發 LLM 應用程式的架構) 套用一些摘要化策略。本記事本涵蓋多種如何摘要化大型文件的範例。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXsvgIuwTPZw"
      },
      "source": [
        "### 目標\n",
        "\n",
        "在這個教學課程中，你將學習如何使用 LangChain 搭配 PaLM API 透過執行以下範例來摘要大型文件：\n",
        "\n",
        "- 填充法\n",
        "- MapReduce 方法\n",
        "- 改善法\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skXAu__iqks_"
      },
      "source": [
        "### 成本\n",
        "\n",
        "此教學使用 Google Cloud 的計費元件：\n",
        "- Vertex AI Generative AI Studio\n",
        "\n",
        "進一步了解 [Vertex AI 價格](https://cloud.google.com/vertex-ai/pricing)，並使用 [價格計算器](https://cloud.google.com/products/calculator/) 根據你的預計使用量產生預估成本。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvKl-BtQTRiQ"
      },
      "source": [
        "## 開始使用\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwFMpIMrTV_4"
      },
      "source": [
        "### 安裝 Vertex AI SDK 和其他依賴項\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8aP6JVlZkS-m"
      },
      "outputs": [],
      "source": [
        "!sudo apt -y -qq install tesseract-ocr\n",
        "!sudo apt -y -qq install libtesseract-dev\n",
        "!sudo apt-get -y -qq install poppler-utils #required by PyPDF2 for page count and other pdf utilities\n",
        "!sudo apt-get -y -qq install python-dev libxml2-dev libxslt1-dev antiword unrtf poppler-utils pstotext tesseract-ocr flac ffmpeg lame libmad0 libsox-fmt-mp3 sox libjpeg-dev swig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oDmNq5__Trl4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "if not os.getenv(\"IS_TESTING\"):\n",
        "    USER = \"--user\"\n",
        "else:\n",
        "    USER = \"\"\n",
        "\n",
        "! pip3 install {USER} --upgrade pytesseract pypdf PyPDF2 textract langchain transformers google-cloud-aiplatform --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWwtjLV5TY6H"
      },
      "source": [
        "**Colab 專屬** : 執行以下Cell，重新啟動核心。對於 Vertex AI Workbench，你可以使用最上方的按鈕重新啟動終端機。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XRvKdaPDTznN"
      },
      "outputs": [],
      "source": [
        "# Automatically restart kernel after installs so that your environment can access the new packages\n",
        "import IPython\n",
        "\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opUxT_k5TdgP"
      },
      "source": [
        "### 驗證你的筆記本環境\n",
        "\n",
        "- 如果你使用 **Colab** 來執行這個筆記本，請執行以下單元格並繼續。\n",
        "- 如果你使用的是 **Vertex AI Workbench** ，在此處查看設定說明 [link](https://github.com/doggy8088/generative-ai/tree/main/setup-env).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vbNgv4q1T2Mi"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5fXfvzhTkYN"
      },
      "source": [
        "### 匯入函式庫\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**僅限 Colab：** 執行下列Cell以初始化 Vertex AI SDK。對於 Vertex AI Workbench，不需要執行此操作。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GjSsu6cmUdEx"
      },
      "outputs": [],
      "source": [
        "import vertexai\n",
        "\n",
        "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\n",
        "REGION = \"us-central1\"  # @param {type:\"string\"}\n",
        "\n",
        "vertexai.init(project=PROJECT_ID, location=REGION)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cRkcfnQMT9vD"
      },
      "outputs": [],
      "source": [
        "import urllib\n",
        "import warnings\n",
        "from pathlib import Path as p\n",
        "\n",
        "import pandas as pd\n",
        "from langchain import PromptTemplate\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.llms import VertexAI\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAGaTjPVTmhP"
      },
      "source": [
        "### 載入模型\n",
        "\n",
        "你載入的名為 `text-bison@001` 的預訓練文字生成模型。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ITUmZiNZcMUW"
      },
      "outputs": [],
      "source": [
        "vertex_llm_text = VertexAI(model_name=\"text-bison@001\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKG-ZTJ_02wq"
      },
      "source": [
        "## 大文件摘要\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZkLDRTjTcfm"
      },
      "source": [
        "### 準備資料檔案\n",
        "\n",
        "為開始，你將需要下載幾個必備於以下摘要作業的檔案。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7H0zINHpTaSu"
      },
      "outputs": [],
      "source": [
        "data_folder = p.cwd() / \"data\"\n",
        "p(data_folder).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "pdf_url = \"https://services.google.com/fh/files/misc/practitioners_guide_to_mlops_whitepaper.pdf\"\n",
        "pdf_file = str(p(data_folder, pdf_url.split(\"/\")[-1]))\n",
        "\n",
        "urllib.request.urlretrieve(pdf_url, pdf_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JELITHdBhnf0"
      },
      "source": [
        "### 從 PDF 中擷取文字\n",
        "\n",
        "使用 `PdfReader` 從掃描文件擷取文字。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x3INtovxreI_"
      },
      "outputs": [],
      "source": [
        "pdf_loader = PyPDFLoader(pdf_file)\n",
        "pages = pdf_loader.load_and_split()\n",
        "print(pages[3].page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDVwBFSjZ7ws"
      },
      "source": [
        "## 方法 1：填入\n",
        "\n",
        "填入是最簡單的方法可將資料傳遞給語言模型。它將文字「塞入」提示中，當作脈絡，如此一來，模型就能夠處理所有相關資訊以取得你要的內容。\n",
        "\n",
        "在 LangChain 中，你可以將 `StuffDocumentsChain` 作為 `load_summarize_chain` 方法的一部分使用。你需要做的就是設定鏈的 `chain_type` 為 `stuff`。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhEi-XqKnv2v"
      },
      "source": [
        "### 指令設計有`填充`鏈\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-ljajUen1YO"
      },
      "outputs": [],
      "source": [
        "prompt_template = \"\"\"Write a concise summary of the following text delimited by triple backquotes.\n",
        "              Return your response in bullet points which covers the key points of the text.\n",
        "              ```{text}```\n",
        "              BULLET POINT SUMMARY:\n",
        "  \"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template=prompt_template, input_variables=[\"text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5aVrDWkJs3Y"
      },
      "source": [
        "### 重試\n",
        "使用 `stuff` 方法啟動鏈並處理三頁的文件。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N_hoizIgObe9"
      },
      "outputs": [],
      "source": [
        "stuff_chain = load_summarize_chain(vertex_llm_text, chain_type=\"stuff\", prompt=prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q1_zwxwgTnlV"
      },
      "outputs": [],
      "source": [
        "three_pages = pages[:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0jEUfOn7UFI2"
      },
      "outputs": [],
      "source": [
        "three_pages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QnXUwWxkrLu4"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    print(stuff_chain.run(three_pages))\n",
        "except Exception as e:\n",
        "    print(\n",
        "        \"The code failed since it won't be able to run inference on such a huge context and throws this exception: \",\n",
        "        e,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKb_fBEedZqu"
      },
      "source": [
        "正如你可以看到的，使用` stuff `方法，你可以透過一次 API 呼叫傳遞所有數據來總結整個文件內容。\n",
        "\n",
        "取決於 LLM 的語境長度，`stuff` 方法將無法正常工作，因為它會導致提示大於語境長度。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RtgemmBzkddX"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    print(stuff_chain.run(pages))\n",
        "except Exception as e:\n",
        "    print(\n",
        "        \"The code failed since it won't be able to run inference on such a huge context and throws this exception: \",\n",
        "        e,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vtp21WX3T7d_"
      },
      "source": [
        "如預期，程式碼回傳預期訊息。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqZrKM32h-o2"
      },
      "source": [
        "### 考量事項\n",
        "\n",
        "「填充」方法是一種摘要文件的方法，在單一呼叫中將整個文件傳輸到大型語言模型 (LLM)。此方法有優點也有缺點。\n",
        "\n",
        "填充方法僅需要呼叫 LLM 一次，這比需要多次呼叫的其他方法要快。在摘要文本時，LLM 可以一次存取所有資料，這可能會產生更好的摘要。\n",
        "\n",
        "但是，LLM 有上下文長度，這是單次呼叫中可處理的最大代幣數。如果文件大於上下文長度，則填充方法將不起作用。而且，填充方法不適合摘要大型文件，因為它很慢，而且可能無法產生好的摘要。\n",
        "\n",
        "讓我們探討其他方法，以協助處理比 LLM 的背景長度限制還要長的文字。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RM3V1JARZ9-k"
      },
      "source": [
        "## 方法 2: MapReduce\n",
        "\n",
        "`MapReduce` 方法實作多階段摘要。它是透過先摘要較小的文字塊，然後將這些摘要組合成單一摘要摘要大型文字區塊的一種技術。\n",
        "\n",
        "在 LangChain 中，你可以在 `load_summarize_chain` 方法中使用 `MapReduceDocumentsChain`。你需要做的是將鏈的 `chain_type` 設定為 `map_reduce`。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lagLXEamlPY2"
      },
      "source": [
        "### 使用 `MapReduce` 鏈設計提示\n",
        "\n",
        "在我們的範例中，有一個 32 頁的文件，你需要摘要。\n",
        "\n",
        "使用 LangChain，`map_reduce` 鏈最多將文件分成 1024 個トークン塊。接著，在每個塊上執行你定義的初始提示來產生該塊的摘要。在下面的範例中，使用了以下第一階段或對應提示。\n",
        "\n",
        "```撰寫以下三引號分隔的文字的簡潔摘要。以涵蓋文字要點的項目符號回覆。\n",
        "'''{text}'''。項目符號摘要：```\n",
        "\n",
        "一旦所有塊的摘要產生後，就會執行不同的提示，將這些摘要合併成單一摘要。在下面的範例中，使用了以下第二階段或合併提示。\n",
        "\n",
        "```撰寫包含所有個別摘要要點的整個文件摘要。```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R6oHEtdSmsTn"
      },
      "outputs": [],
      "source": [
        "map_prompt_template = \"\"\"\n",
        "                      Write a summary of this chunk of text that includes the main points and any important details.\n",
        "                      {text}\n",
        "                      \"\"\"\n",
        "\n",
        "map_prompt = PromptTemplate(template=map_prompt_template, input_variables=[\"text\"])\n",
        "\n",
        "combine_prompt_template = \"\"\"\n",
        "                      Write a concise summary of the following text delimited by triple backquotes.\n",
        "                      Return your response in bullet points which covers the key points of the text.\n",
        "                      ```{text}```\n",
        "                      BULLET POINT SUMMARY:\n",
        "                      \"\"\"\n",
        "\n",
        "combine_prompt = PromptTemplate(\n",
        "    template=combine_prompt_template, input_variables=[\"text\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXoz0uLDMoWD"
      },
      "source": [
        "### 使用 MapReduce 方法產生摘要\n",
        "\n",
        "在定義提示之後，請初始化關聯的 `map_reduce_chain`。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VRGJcBZeVdEa"
      },
      "outputs": [],
      "source": [
        "map_reduce_chain = load_summarize_chain(\n",
        "    vertex_llm_text,\n",
        "    chain_type=\"map_reduce\",\n",
        "    map_prompt=map_prompt,\n",
        "    combine_prompt=combine_prompt,\n",
        "    return_intermediate_steps=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6fekDDr0hrJ"
      },
      "source": [
        "然後，使用鏈條產生摘要。注意 LangChain 預設使用一個 Token 數量限制為 1024 的 Token 分詞器 (來自 transformer 函式庫)。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uSC6w2TBV35q"
      },
      "outputs": [],
      "source": [
        "map_reduce_outputs = map_reduce_chain({\"input_documents\": pages})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "meH2ELuz2H46"
      },
      "source": [
        "在摘要產生後，你可以透過整理輸入文件與其相關的輸出，在 Pandas Dataframe 中驗證它們。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r6FRSR7xRLew"
      },
      "outputs": [],
      "source": [
        "final_mp_data = []\n",
        "for doc, out in zip(\n",
        "    map_reduce_outputs[\"input_documents\"], map_reduce_outputs[\"intermediate_steps\"]\n",
        "):\n",
        "    output = {}\n",
        "    output[\"file_name\"] = p(doc.metadata[\"source\"]).stem\n",
        "    output[\"file_type\"] = p(doc.metadata[\"source\"]).suffix\n",
        "    output[\"page_number\"] = doc.metadata[\"page\"]\n",
        "    output[\"chunks\"] = doc.page_content\n",
        "    output[\"concise_summary\"] = out\n",
        "    final_mp_data.append(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dA9cnh8YaNbF"
      },
      "outputs": [],
      "source": [
        "pdf_mp_summary = pd.DataFrame.from_dict(final_mp_data)\n",
        "pdf_mp_summary = pdf_mp_summary.sort_values(\n",
        "    by=[\"file_name\", \"page_number\"]\n",
        ")  # sorting the dataframe by filename and page_number\n",
        "pdf_mp_summary.reset_index(inplace=True, drop=True)\n",
        "pdf_mp_summary.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yA0eM1K3cvH2"
      },
      "outputs": [],
      "source": [
        "index = 3\n",
        "print(\"[Context]\")\n",
        "print(pdf_mp_summary[\"chunks\"].iloc[index])\n",
        "print(\"\\n\\n [Simple Summary]\")\n",
        "print(pdf_mp_summary[\"concise_summary\"].iloc[index])\n",
        "print(\"\\n\\n [Page number]\")\n",
        "print(pdf_mp_summary[\"page_number\"].iloc[index])\n",
        "print(\"\\n\\n [Source: file_name]\")\n",
        "print(pdf_mp_summary[\"file_name\"].iloc[index])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROrE1-HKpg7y"
      },
      "source": [
        "### 考量\n",
        "\n",
        "有了 `MapReduce` 方法，該模型能以平行處理克服 `Stuffing` 方法的脈絡限制，進而摘要長篇文件。\n",
        "\n",
        "然而，`MapReduce` 需要多次呼叫模型，並有可能在頁面之間失去脈絡。\n",
        "\n",
        "要解決這個問題，你可以嘗試另一種方法來一次摘要多個頁面。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxdB-5PqgCf-"
      },
      "source": [
        "## 方法 3：Refine\n",
        "\n",
        "Refine 方法是處理大型文件摘要的另一種方法。其運作方式是先在少量資料上執行初始提示，生成一些輸出。然後，針對後續的每一份文件，會輸入前一文件的輸出和新文件，並要求 LLM 根據新文件修正輸出。\n",
        "\n",
        "在 LangChain 中，你可以在 load_summarize_chain 方法中使用 `MapReduceDocumentsChain`。你需要做的就是將鏈的 `refine` 設定為 `chain_type`。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjj2UZilDF4Q"
      },
      "source": [
        "### 以 `Refine` 鏈設計提示\n",
        "\n",
        "使用 LangChain，`refine` 鏈需要兩個提示。\n",
        "\n",
        "用於產生後續任務輸出之問題提示。用於根據產生之內容修正輸出的修正提示。\n",
        "\n",
        "在此範例中，問題提示為：\n",
        "\n",
        "```\n",
        "請提供以下文字之摘要。\n",
        "TEXT: {text}\n",
        "SUMMARY:\n",
        "```\n",
        "\n",
        "而修正提示為：\n",
        "\n",
        "```\n",
        "用三個反引號分隔，撰寫以下文字的簡潔摘要。\n",
        "以項目符號回覆，涵蓋文字中的重點。\n",
        "```{text}```\n",
        "項目符號摘要：\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XiZX45Z5VTwS"
      },
      "outputs": [],
      "source": [
        "question_prompt_template = \"\"\"\n",
        "                  Please provide a summary of the following text.\n",
        "                  TEXT: {text}\n",
        "                  SUMMARY:\n",
        "                  \"\"\"\n",
        "\n",
        "question_prompt = PromptTemplate(\n",
        "    template=question_prompt_template, input_variables=[\"text\"]\n",
        ")\n",
        "\n",
        "refine_prompt_template = \"\"\"\n",
        "              Write a concise summary of the following text delimited by triple backquotes.\n",
        "              Return your response in bullet points which covers the key points of the text.\n",
        "              ```{text}```\n",
        "              BULLET POINT SUMMARY:\n",
        "              \"\"\"\n",
        "\n",
        "refine_prompt = PromptTemplate(\n",
        "    template=refine_prompt_template, input_variables=[\"text\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-USlaSPbM0rs"
      },
      "source": [
        "### 使用 Refine 方法生成摘要\n",
        "\n",
        "在你定義提示後，可以使用「refine」鏈類型啟動摘要鏈。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_-Sv3HO1U3hi"
      },
      "outputs": [],
      "source": [
        "refine_chain = load_summarize_chain(\n",
        "    vertex_llm_text,\n",
        "    chain_type=\"refine\",\n",
        "    question_prompt=question_prompt,\n",
        "    refine_prompt=refine_prompt,\n",
        "    return_intermediate_steps=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9EZCDK-MQJH"
      },
      "source": [
        "然後，你使用總結鏈，使用「精煉」方法來總結文件。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KHwwab7vXNa1"
      },
      "outputs": [],
      "source": [
        "refine_outputs = refine_chain({\"input_documents\": pages})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUqpki5EMYEr"
      },
      "source": [
        "以下你可以看到總結的結果。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7j5cUGStZ5WF"
      },
      "outputs": [],
      "source": [
        "final_refine_data = []\n",
        "for doc, out in zip(\n",
        "    refine_outputs[\"input_documents\"], refine_outputs[\"intermediate_steps\"]\n",
        "):\n",
        "    output = {}\n",
        "    output[\"file_name\"] = p(doc.metadata[\"source\"]).stem\n",
        "    output[\"file_type\"] = p(doc.metadata[\"source\"]).suffix\n",
        "    output[\"page_number\"] = doc.metadata[\"page\"]\n",
        "    output[\"chunks\"] = doc.page_content\n",
        "    output[\"concise_summary\"] = out\n",
        "    final_refine_data.append(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N_7Mm9cEmGOV"
      },
      "outputs": [],
      "source": [
        "pdf_refine_summary = pd.DataFrame.from_dict(final_refine_data)\n",
        "pdf_refine_summary = pdf_mp_summary.sort_values(\n",
        "    by=[\"file_name\", \"page_number\"]\n",
        ")  # sorting the datafram by filename and page_number\n",
        "pdf_refine_summary.reset_index(inplace=True, drop=True)\n",
        "pdf_refine_summary.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jvLVCs8Gbwbw"
      },
      "outputs": [],
      "source": [
        "index = 3\n",
        "print(\"[Context]\")\n",
        "print(pdf_refine_summary[\"chunks\"].iloc[index])\n",
        "print(\"\\n\\n [Simple Summary]\")\n",
        "print(pdf_refine_summary[\"concise_summary\"].iloc[index])\n",
        "print(\"\\n\\n [Page number]\")\n",
        "print(pdf_refine_summary[\"page_number\"].iloc[index])\n",
        "print(\"\\n\\n [Source: file_name]\")\n",
        "print(pdf_refine_summary[\"file_name\"].iloc[index])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dwgbRTrM5Cb"
      },
      "source": [
        "### 考慮\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1H0Y5pPcXbgm"
      },
      "source": [
        "簡單來說，採用 LLM 的文字摘要 Refine 方法可以匯入更多相關脈絡，且資料損失可能比 Map Reduce 少。不過，與 Stuffing 相比，Refine 需要更多次呼叫 LLM，而且這些呼叫不是獨立的，表示它們無法平行處理。此外，對於文件的排序順序也有一定的依賴性。最新的文件可能變得更相關，因為此方法容易受 recency bias 影響。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAaWXncPMhv4"
      },
      "source": [
        "## 結論\n",
        "\n",
        "\n",
        "在本筆記本中，你將了解使用 LangChain 和 PaLM API 總結長篇文件的方法。你在本筆記本中看到的方法是你能使用的可能性之一。例如，還有另一種稱爲Map-Rerank 方法，它包括在每個數據塊上運行一個初始提示，不僅嘗試完成一個任務，而且還給出其答案如何確定的評分。然後根據此評分對響應進行排序，並返回最高評分。\n",
        "\n",
        "話雖如此，重要的是強調，根據你的需要，你可以考慮使用純粹基礎模型，並使用自訂框架來建立生成式 ai 應用程式。\n",
        "\n",
        "以下是使用帶有自訂框架的基礎模型的一些好處：\n",
        "\n",
        " - 更靈活地使用不同的 LLM、提示範本、文件處理策略等來實現應用程式。\n",
        "\n",
        " - 更能根據你的情境自訂生成式應用程式。\n",
        "\n",
        " - 更佳的效能以改善應用程式的延遲和可擴充性。\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "summarization_large_documents_langchain.ipynb",
      "toc_visible": true
    },
    "environment": {
      "kernel": "python3",
      "name": "tf2-gpu.2-11.m108",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-11:m108"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}